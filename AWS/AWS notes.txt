----------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------IAM - CLI-----------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
3 ways to access AWS
- via console
- CLI by access keys
- SDK by access keys

Access keys are generated through the AWS console and managed by a User

IAM security tools:
- IAM credentials Report (account-level)
- IAM Access Advisor

AWS CLI Profile:
    In CLI how can we deal with multiple accounts? --profile.
    Usually, we log in to an account by `$aws configure`. And in this case we log in to the account as default
    But we can log in as separated account by `$aws configure --profile < profile name you want>`
    And we can use the new account in this way: `aws s3 ks --profile <the name>`

MFA in CLI
    To log in via MFA we need to create a STS temporary session by MFA credentials. And every logging in we use the session token (129 - lesson)
    `aws sts get-session-token --seral-number <it is available in user credentials> --token-code <generated by your MFA device>`

--------------------------------------Cost Management--------------------------------------------------
in root account activate "IAM user/role access to billing information" to monitor billing in an IAM user

- create budget to alert when a target exceeds

------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------EC2------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------
!!! I am gonna back to types of instance and purchase of instance soon, lesson 131, 132,133, Event bus, registry code

EBS - is like USB storage but not physical, it is locked to an Availability Zone
- It uses network to communicate the instance, which means there might be a bit of latency
- It can be detached from an EC2 instance and attached to another one quickly
- An EBS volume in us-east-1a cannot be attached to us-east-1b
- To move a volume across, you first need to snapshot it
- You can take a snapshot from a EBS without detaching, but not doing it is recommended

EBS Snapshot Features:
- EBS Snapshot Archive: make your snapshot archive tier that is 75% cheaper
- But takes 24-72 hours to be restoring the archive
- Recycling Bin for EBS snapshot: you can specify a retention period for a snapshot (1 day - 1 year)
- Fast Snapshot Restore: Force full initialization of snapshot to have no latency on the first use by more CASH $$$

EBS types
EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec)

- gp2/gp3: general purpose SSD volume that balance price and performance                                
    - Cost effective and low latency
    - System boot volumes, Virtual desktops
    - 1GB - 16TB
    - gp3 is newer generation of gp2
    - in gp2 volume and IOPS are linked while in gp3 they are independent
    - means: 3 IOPD per GB, at 5334 GB we are at the max IOPS (16000) in gp2 (baseline: 3000 IOPS)
    - in gp3 baseline IOPS 3000 and throughput 125 MB/s,  can increase up to 16000 IOPS and 1000MB/s tp independently

- Provisioned IOPS or PIOPS:
- io1/io2: highest-performance SSD volume for mission-critical low-latency or high-throughput workloads | used as boot volumes
    - Critical business application with sustained IOPS performance |
    - Or applications that need more than 16000 IOPS                | - 4 Gb-16 TB. max: 64k PIOPS for Nitro EC2 & 32k for others
    - Independently                                                 | 
- io2 Block Express 4GB to 64 TB: very fast max: 256k PIOPS with an IOPS:GB ratio of 1000:1

- st1: low cost HDD volume designed for frequently accessed, throughput-intensive workloads
- sc1: lowest cost HDD volume designed for less frequently accessed workloads
- Supports EBS Multi-attach

EBS Multi Attach:
- Up to 16 Ec2 Ins can have a volume (only io1/io2)
- It looks like several applications are connected to a general EBS
- All instances and volume should be in the same AZ

AIM
- You can take a image by customizing an instance
- or you can buy it from Amazon marketplace
- By creating Image you can create a new instance from it

EC2 Instance store
- as an alternative to Volumes, you can leverage EC2 instance store. It is a peace of the instance not another part
- Better I/O performance
- But the data is gonna be destroyed when the instance is stopped
- Good for buffer / cache/ scratch data / temp content
- Risk of data loss if hardware fails
- Backups and Replications are your responsibility


EC2 Metadata:
    - ec2 instance can see easily see its metadata by several API requests. By just 2 step:
    - TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"` (specifying TOKEN)
    - curl -H "X-aws-ec2-metadata-token: $TOKEN" -v http://169.254.169.254/latest/meta-data/{metadata path}
    - e.g. if IAM role is associated to your ec2 instance you can get the token by requesting: 
            curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/identity-credentials/ec2/info/security-credentials/



##############################################################
Load Balancer
- There are 3 types of load Balancer: Classic (out of date), Application, Network, Gateway Load Balancers.
    - ALB: layer 7 - receives HTTP/HTTPS protocols and get them to target addresses (instances, IPs etc)
    - NLB: layer 4 - receives TCP/UDP protocols and get them to target addresses (instances, IP, ALBs, etc)
    - GLB: receives requests and get them 3rd party system, if everything is good, the requests are sent to main Applications (target addresses)

- Sticky Session (Session Affinity) - the same client is redirected to the same server even by load balancer to take advantage from cookies, etc
                                                                                      !!! to configure stickiness edit Target Group attributes!!!
        - Application-based Cookies
            - Custom Cookies:        created by application for its own optimization
            - Application Cookies:    created bt LB
        
        - Duration-based Cookies: Generated by LB

When using an Application Load Balancer to distribute traffic to your EC2 instances, the IP address you'll receive
requests from will be the ALB's private IP addresses. To get the client's IP address, ALB adds an additional header
called X-Forwarded-For contains the client's IP address.

You can configure the Auto Scaling Group to determine the EC2 instances' health based on Application Load Balancer
Health Checks instead of EC2 Status Checks (default). When an EC2 instance fails the ALB Health Checks, its marked
unhealthy and will be terminated while the ASG launches a new EC2 instance.

There's no CloudWatch Metric for "requests per minute" for backend-to-database connections. You need to create a
CloudWatch Custom Metric, then create a CloudWatch Alarm.

For each Auto Scaling Group, there's a Cooldown Period after each scaling activity. In this period, the ASG doesn't
launch or terminate EC2 instances. This gives time to metrics to stabilize. The default value for the Cooldown
Period is 300 seconds (5 minutes).

##############################################################
Cross-Zone Load Balancer
- If this feature is enabled, load balancer will balance the traffic among all targets regardless of the AZ
- If disabled, all AZs get the same amount of traffic and its (AZ's) servers take equally the traffic which is directed to AZ
- It is by default enabled in ALB and does not charge, while disabled in NLB and GLB by default, and charges if enabled
- You can configure it in Target Group level 

##############################################################
Connection Draining (Deregistration Delay)
- Let's say our instance is out of service due to in-flight requests or taking scaling processes, in this case the ALB/NLB identifies the instance is unhealthy
    So stops sending new requests to it. And waits for the time which specified (by default 300 seconds) 0 to 3600 can be chosen

-----------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------EFS------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------
EFS is an elastic file system which is connected via Network (Network Fie System) with
- It can be associated with EC2 instance when creation
- Whe you are creating a EC2 ins, by default, other security groups are gonna be created by system and associated 
to the EFS and EC2 (instance-sg-1 > instance | efs-sg-1 > efs) and they maintain the connection
- you can find an outbound rule in the sg which addresses to the EFS server
- you cannot connect with your EFS without the rule (port is 2049)





-------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------RDS------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------

RDS is Relational Database Service of AWS. It can involve Oracle DB, SQL Server, MYSQL, Postgres, Aurora
- Primary DB is used for all transactions and connected with application

READ REPLICA
- You can have up to 15 Read replicas in RDS (15 for Aurora as well)
- Read Replica is used for only read operations (analyze, BI)
- Data is replicated from Primary to Read Replica in ASYNC method
- Unlike other services, RDS does not charges for inter AZ activities. 
- But you will be charged for Cross Regional activities

MULTI AZ (Disaster Recover)
- It is backup database to which data is replicated SYNC to immediately reflect data 
- It is good when one one AZ is bombed your data is available in other AZ
- !!! A Read Replica can also set up as a Multi AZ RDS
- Primary and Multi AZ RDBs have the same domain so that there is no need any action to tie your app with new backup DB when primary is bombed 


!! Oracle does not support IAM database authentication 

------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------ElastiCache--------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------
Like RDS you can enable multi AZ for ElastiCache as well

        METHODS:

1. Lazy Loading / Cache Aside / Lazy Population
    - Cache hit: If app get data from cache it is ok. it is called cache hit
    - Cache miss: If add can't get data from cache due to absence
    - Read from DB: after above case, the app get data from DB
    - Write to cache: And white the data to the cache

2. Write-Through
    - If some data is added or updated the it will be also done in Cache
    - You can take more advantage if integrate this method with the Lazy load
    - TTL is not good choice with this method

Cache Eviction methods
1. Explicitly
2. Evicting by LRU (least Recently Used) when a metric reach a top
3. Time-to-Live (TTL). Setting a period in which data is gonna be retained in the cache
(If too many evictions piss off you, you can scale up or out your ElastiCache)

5 - the maximum number of Read Replicas you can add in an ElastiCache Redis Cluster with Cluster-Mode Disabled?

????????????????????????????
A source DB instance can have cross-Region read replicas in multiple AWS Regions
A Read Replica in a different AWS Region than the source database can be used as a standby database and promoted
to become the new production database in case of a regional disruption. So, we'll have a highly available (because
of Multi-AZ) RDS DB Instance in the destination AWS Region with both read and write available.
????????????????????????????


----------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------Route 53---------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------
When we go 

Alias names don't route to EC2 instances
ALIAS does not charge
We cannot use root domain / APEX in CNAME. But can it in ALIAS
    for example: we have a domain: umidjon.com, and it should route to ALB. CNAME cannot use umidjon.com as a domain but can app.umidjon.com
    But the with ALIAS, we can assign a ELB or CloudFront, etc to the root domain

Record Types:
    - A:         Maps a domain or subdomain to IPv4
    - AAAAAA:    Maps a domain or subdomain to IPv4
    - CNAME:     Routes to another domain
    - ALIAS:     Routes to a AWS resource  (for A and AAAAAA record types)

DNS cache:
    - TTL. We can specify TTL in which client remember that certain domain maps to certain IP
    - ALIAS: we cant specify explicitly TTL for ALIAS

Routing Policies:
    - Simple: 
        - typically specify a record. But can include multiple targets (IPs), if so client choose a record randomly.
        - If ALIAS, it returns only a aws resource
        - Can't be attached Health check

    - Weight:
        - you will distribute traffics among records by setting weights to records. 1-255. 
        - Record names and types must be the same.
        - Can be attached Health check
    
    - Latency Based
        - Redirects the client to the resource that has the least latency
        - Latency is based on traffic between users and AWS Region
        - It is good when latency is crucial
        - Can be associated with health check
    
    - Failover
        - There are 2 types of records: primary and secondary
            - Primary: mandatory to specify Health Checks
            - secondary: optional to specify Health Checks. It is Failover record in case of primary fails
    
    - Geolocation
        - You associate a resource with a Continent / Country / US States
        - Can associate HCH
        - Must specify a resource as a default
    
    - Geoproximity (using Route 53 Traffic Flow feature)
        - Route traffic to resources based on the geographic location of users and resources
        - Unlike Geolocation policy, this has ability to shift more traffic to resource based on defined bias
        - bias can be 1 to 99 and -1 to -99 to expand or shrink traffic to resource

    - IP based
        - Routes based on client's IP (CIDR range) to specific resource which you specified
    
    - Multi-Value Answer
        - Returns multi healthy IPs/resources 

Health Check
    - We can prevent our clients from being redirected to unhealthy resource (mainly Public)
    - To do that, during creation of record, we need to associate a Health Check
    - Health Checks are integrated with CW metrics
    - Multiple HCHs the all around world can check the endpoint
        - HTTPS/HTTP/TPC
        - if > 18% of health checkers report the endpoint is healthy, the service consider it is healthy, otherwise unhealthy
        - you can set parameters healthy/unhealthy threshold. It says about status after a certain number (which you set) of consecutive checks 
    - your resource is OK if HCH returns 2xx or 3xx status
    - configure your router/firewall to allow incoming request from Route 53 Health Checkers
    - you can combine child HCHs (AND, OR, NOT) to make decision of Parent HCH
    - To set health checkers for the private resources, first you need to attach CW to the resource and tie the HCH with the CW Alarms


    Types of Health Checks:
        - HCH that monitors an endpoint
        - HCH that monitors other Health Checks
        - HCH that monitors Cloud Watch Alarm

-----------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------VPC------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------------
VPC: provides network isolation, allowing you to create a private, dedicated portion of the cloud for your resources.
    This isolation is crucial for security and privacy.

Subnets: Within a VPC, you can create subnets to further organize and segment your resources. Subnets are associated
    with specific availability zones within a region.

    Subnets can be divided into 2 types:
    - Public: Directly goes to internet by internet gateway
    - Private: Cannot communicate with external internet. Only way to communicate internet is that
       You will create NAT gateway / instance in public Subnet, and it directs the private subnet to the internet via Internet Gateway

SG (Security Group) & NACL (Network Access Control Lists):
    - NACL is subnet level. Include allow & block list. only ip can be specified. Stateless
    - SG is instance level. Include only allow list. Can contain IP and other SG. Stateful

    Traffic first come to NACL, if passed come to SG.
    All instance in a subnet are set NACL of the subnet automatically

VPC Flow Logs
    Every traffic log to a VPC is recorded to VPC Flow Log
    Logs of VPC, Subnet or Elastic Network Interface (ENI)
    Helps to monitor / troubleshoot connectivity issues. Debugging
    The logs can be sent to S3, CloudWatch Logs, and Kinesis data Firehose
    Captures network information from AWS managed interfaces too: ELB, ElastiCache, RDS, Aurora, etc

VPC Peering
    To connect resources between VPSs we need explicitly assign VPC Peering between the two VPCs which we are connecting
    Must not have overlapping CIDR. 

VPC Endpoints
    Usually all services in AWS are publicly, but what if a service is private?
    Let's say we have a EC2 instance in private submit and how can we connect to it on behalf a random AWS Service?
        - via VPC Endpoint Gateway   - for S3 and DynamoDB
        - via VPC Endpoint Interface - the rest
    In that case you will connect to the EC2 privately, securely, within the VPC, without latency. 
        That is why you need to use VPC endpoints instead of connecting from outside (www) via NET Gateway (etc)

Connecting to VPC
    - Site to site VPN
        - It provides  connection btw your office (for example) and VPC by public connection (www)
        - Connection is encrypted automatically

    - Direct Connect (DX)
        - Establishing physical connectivity btw on-premises and aws
        - It is totally private, secure and fast
        - Goes through a private network
        - Takes at least a months to establishing
    





------------------------------------------------------------------------------------------------
=================================IP/CIDR========================================================
------------------------------------------------------------------------------------------------
Public IP is provided by your internet service provider.
Local IP is provided by your router. By the local IP you can connect with co-networkers easily (you can play game)
IPv4 is from 0.0.0.0 to 255.255.255.255 in human eyes. But why 255? because 255 is maximum value of 8-bit binary
In fact, the range is from 00000000.00000000.00000000.00000000 to 11111111.11111111.11111111.11111111

what is CIDR tho? CIDR is a protocol defines in which range the router (or other provider) give you and co-networkers IPs
for example: 192.132.01.0/24 means that your IP provider can give all users in range of from 192.132.01.0 to 192.132.01.255
/24 means the first 24-bit remains the same, but remain 8 bits can be changed.
if we define the IP as binary: 11000000.10000100.00000001.00000000
                               \________24-bit__________/ \_8-bit_/
your router keeps the 24 bit the same and remain 8 bits are can be changed dynamically




------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------S3------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Features and Performance
    Buckets are region level but its name must be globally unique. S3 itself is global service
    bucket.folder1/subfolder1/object.pdf
        \_________________/       \_/
               prefix           suffix
    max size of object can be 5TB.
    suitable at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket
    no prefix limits
    Multi-Part upload recommended for 100MB+ and must be used for 5GB+ files (it optimizes upload by parallelism)
    Byte-Range Fetches do almost the same functionality with Multi-Part but for download. divides into bytes and requests to local machine parallel
        Can be used to retrieve only partial data very fast
    S3 Transfer Acceleration increases upload speed by using Edge Location as a middleman
    S3 Select and Glacier Select filters data within the s3 and retrieve it to the app:
        Regular apps: {Extract} => Filter => Load or {Extract} => Load => Filter. in this case your app goes down
        SQL Select and Glacier Select: {Extract&Filter} => Load . up to 400% faster and 80% cheaper
    Presigned URL. In Object action you can generate presigned url for an object
    Access Points. You can create AP like bucket policy. It allows user to use specific resource in the bucket

Security
    - We can set bucket or object policy to control access into the bucket/object
    - Can be assigned policy by IAM policy
    - Encryption can be enabled
    - Block Public Access can be enabled

We can enable static website hosting in directory level
We can enable versioning in bucket level

Replication: (Go bucket's Management section)
    - Must enable versioning in source and destination buckets
    - Cross-Region Replication (CRR)
    - Same-Region Replication (SRR) 
    - Buckets can be in different AWS account
    - Copying is asynchronous
    - Must give proper IAM permissions to S3
    - After enabling replication, only new objs are replicated
        - to replicate existing objs, use S3 Batch Replication
    - Deletes are not replicated
        - delete markers can be replicated if explicitly enabled
        - but still permanent deletes is not be applied

S3 Storage Classes: (By life cycle you can automate changing btw storage classes based on a certain condition)
    - Amazon S3 Standard                        : 99.99 availability, for frequently accessed data, low latency and high through. UC: Big data, game apps
    - Amazon S3 Standard-infrequent Access (AI) : 99.99 availability, Lower cost, for infrequently accessed data. UC: backup, Disaster Recovery
    - Amazon S3 One Zone-infrequent Access      : High durability (99.9999999999%) in single AZ. 99.5 availability. data lost when AZ is bombed. UC: second backup
            
            Glacier Storages: Low cost of data (archival). Price for storage+retrieving
    - Amazon S3 Glacier Instant Retrieval       : Milliseconds retrieval, minimum storage duration is 90 days. UC: data accessed once a quarter
    - Amazon S3 Glacier Flexible Retrieval
        - Expedited                             : 1 to 5 mins retrieval     \
        - Standard                              : 3 to 5 hrs retrieval      |  -> min storage duration 90 days
        - Bulk                                  : 5 to 12 hours (free)      /   
    - Amazon S3 Glacier Deep Archive
        - Standard                              : 12 hours retrieval        \
        - Bulk                                  : 48 hours retrieval        /  -> min storage duration 180 days 

    - Amazon S3 Intelligent Tiering             : small monthly monitoring and auto-tiering fee. No retrieval charges, based on usage objects are tiered automatically
        - Frequently access tier                    : default
        - Infrequent access tier                    : not accessed for 30 days
        - Archive Instant access tier               : not accessed for 90 days
        - Archive Access tier (optional)            : configurable from 90 to 700+ days
        - Deep Archive Access tier (optional)        : configurable from 180 to 700+ days


S3 Lifecycle Rules: (Go bucket's Management section)
    - Transitions Actions: configure objects to transition to another storage class
        e.g. you can set storage class to transit automatically after 60 inactive days
    - Expiration actions: configure objects to expire (delete) after some time
       - for log files (can be set 265 days for example)
       - old version of files 
       - incomplete multi-part uploads
       - we can assign it by prefix or tag

S3 Analytics - Storage Class Analysis
    - Help you decide when to transition objects to the right storage class
    - Provide recommendations for Standard and Standard IA
    - Does NOT work for One-Zone or Glacier
    - Report is updated daily

S3 Event Notification (Configure it in Properties section of a Bucket)
    This service sends notification about changes on S3 to SNS, SQS or Lambda Functions (CreateObject, RemoveObject, etc)
    Instead of attaching a role to S3, we have to attach needed policies to that resources
    Can filter objects
    you can use Amazon EventBridge for event purposes, broader service while Event Notification is for only s3.

User-defined Object Metadata and S3 Object Tags
    - You can define a metadata for a object. But it must start with "x-amz-meta-" as key-pairs
    - You can also assign tags to classify your objects
    - you cannot search objects by its metadata or tags. instead, you can use an external DB as a search index such as DynamoDB

Object Encryption (header: "x-amz-server-side-encryption":"<value>")
    
    With Amazon S3 Managed Keys - SSE-S3 (Default)  <value> = AES256                                             \
    With KMS Keys stored in AWS KMS - SSE-KMS       <value> = aws:kms                                            | -> Server-Side Encryption (SSE)
    With Customer-Provided Keys - SSE-C             Key must be provided in headers (HTTPS is mandatory)         /
    Client Side Encryption                          Client encrypts file and upload it
    
    You can specify a logic that says in S3 policy: 'If transit is not secure deny it' - enforces user to use HTTPS
    You can also deny traffic based on your encryption requirements

S3 CORS. Cross-Origin Resource Sharing
    ORIGIN = schema (protocol) + host + port
    For example you request a html file from a web server and as a response the server reference another cross-origin 
        server. If CORS is enabled, the referenced server gives you a html without any problem
    CORS headers must be involved preflight response of the second server. If the client satisfy the headers, it send a http (e.g) request to the server
    In Amazon S3, if client makes a cross-origin request on a S3 bucket, we need to enable the correct CORS headers
    In S3, go to CORS section and configure the json like:
        [
            {
                "AllowedHeaders": [
                    "Authorization"
                ],
                "AllowedMethods": [
                    "GET"
                ],
                "AllowedOrigins": [
                    "<url of first bucket with http://...without slash at the end>"
                ],
                "ExposeHeaders": [],
                "MaxAgeSeconds": 3000
            }
        ]

MFA  Delete
    We can enforce users leveraging MFA to do important operations on S3. For permanent delete and suspending versioning
    To use MFA Delete, Versioning must be enabled on the bucket
    if you want to enable MFA Delete, disable all life cycle rules
    Only root account can enable/disable MFA Delete
    # enable mfa delete
        `$aws s3api put-bucket-versioning --bucket mfa-demo-stephan --versioning-configuration Status=Enabled,
                            MFADelete=Enabled --mfa "arn-of-mfa-device mfa-code" --profile root-mfa-delete-demo`
    After enabling MFA Delete, important actions are done by MFA devices via CLI

Access Log
    We can enable access log on a bucket
    every activity is gonna be logged into the destination bucket
    Do NOT choose current bucket as a destination, else you waste credits by infinity log
    Your destination bucket's policy will be changed. 



------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------CloudFront-------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Also Content Delivery Network (CDN)
Improves read performance by caching at the edge
For example: You have a S3 (any backend origin). When you enable CloudFront Distribution on the S3 address,
    client first looks up cache from the nearest Edge Location. If not found, Cloud Front bring the page to the Edge 
    from the S3 and it is cached during TTL. And it will available for other users
For this, you must create CloudFront distribution (Origin Access Control too) and configure S3's bucket policy to access on an origin
When you call a distribution:
    - it says to Cache Behaviors: "Hey this is url, route this url to needed Origin (backend)" \
    - Cache Behaviors takes the url and routes to specified Origin                             | -> /login -> ec2 (login backend), /* -> s3 (default)
    - And needed origin starts working.                                                        /

Advanced Concepts:
    - CloudFront Edge locations are all around the world
    - The cost of data out per edge location varies 
    - To reduce cost, reduce Edge locations. There are 3 classes:
        - All: all regions are available. Best performance
        - 200: Most regions except for most expensive ones
        - 100: Only the least expensive regions
    - Origin Groups: 
        - to increase high-availability and do failover
        - origin group: one primary and one secondary origin
        - If the primary origin fails, the secondary one is used

Cache Policy
    Contents are cached with their unique identifier. With this UID edge location retrieves exact content from cache.
        Default key is hostname+resource portion
        But you can add other elements (HTTP headers, cookies, query strings) to the cache key using CloudFront Cache Policies
            - HTTP header                   None, Whitelist
            - Cookies                       None, Whitelist, Include All-Except, All
            - query string (after ? mark)   None, Whitelist, Include All-Except, All

Origin Request Policy
    Specify values that you want to include in origin requests without including them in the Cache Key 
    For example, header incudes Secret Key but you don't want to ad it to the Cache key
    You can include: 
        - HTTP header                   None, Whitelist, All viewer headers option
        - Cookies                       None, Whitelist, All
        - query string (after ? mark)   None, Whitelist, All
    Ability to distinguish CloudFront headers and Custom headers (which is not related to caching operations)
    Instead create a custom policy, you can use Predefined managed Policies

CloudFront Invalidations
    Well. You set TTL to refresh Cache. But what is the underlying reference is changed before TTL?
    In that case we can enforce an entire (*) or partial cache (/images/*) to refresh
    This section invalidates specified caches immediately when base is changed

To maximize performance, separate dynamic and static origins

ALB and EC2 as an origin
    ALB must be public and open to edge locations' traffic. In this case EC2 can be private
    EC2 instances must be public and open to edge locations' traffic

Geographic Restrictions
    You can list of countries from which traffic should be allowed/blocked
    It uses third-party service to identify countries's ips

Signed URL / Signed Cookies
    Some files or cookies can be exist that you do not want to be them publicly. Only specific user can access them
    In this case use signed url/cookies. 
    For this, you need to attach a policy with:
        includes URL expiration
        Includes IP ranges to access them from
        Trusted signers (which AWS accounts can create signed URLs)
    
    Signed URL     = access to individual files
    Signed Cookies = access to multiple files

    Signed url uses RSA key to encrypt. Field level Encryption, 
        - Edge location encrypts data by public key, and it is decrypted by private key in last step (EC2 instance) 


------------------------------------------------------------------------------------------------
===============================Encryption & Decryption==========================================
------------------------------------------------------------------------------------------------

Encryption is changing the body of a data to plaint text (ciphertext) (by a E(*arg) function). Decryption is reverse of Encryption (a D(*arg))
let's say we have 2 people Alice and Bob.

Symmetric Encryption:
    Alice sends a message to Bob by E(m)=c encryption. To read Bob decrypts the ciphertext by D(d)=m

Asymmetric Encryption:
    Alice and Bob have a common key. So alice encrypts a message like E(m, key)=c. Bob decrypts like D(c, key)

RSA:
    RSA is asymmetric encryption algorithm that generates public and private keys.
    Public keys can be shared but private must not be done!
    Alice and Bob both have public and private keys:
    Alice to send a message to Bob. Alice encrypts message by E(m, <public key of Bob>) = c
    Bob decrypts it by D(c, <his own private key>).
    
    Without private key you can never decrypt ciphertext.


------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------ECS--------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------

ECS is Elastic Container Service to run Docker containers in AWS
Launch Docker containers on AWS = Launch ECS Tasks on ECS Cluster

EC2 Launch Type:
    you must provision & maintain the infrastructure (EC2 instance)
    There is a ECS Cluster and it contains EC2 Instances. In case of ECS, each instance must run the ECS Agent to register in the ECS Cluster
    Then AWS takes care of starting/stopping containers in EC2 instances
    In ECS, security group's rules does NOT matter

Fargate Launch Type:
    Launch docker containers on AWS too.
    You do not provision the infrastructure (no EC2 instance)
    It's all serverless
    We just create task definitions in ECS Cluster and AWS runs the task for us based on te CPU/RAm you need
      
IAM roles for ECS Tasks
    EC2 Instance Profile:
        Used by the ECS Agent
        Makes API calls to ECS service
        Send container logs to CloudWatch Logs
        Pull docker image from ECR
    
    ECS Task Role:
        Attached roles for each Task in EC2 instance
        Task role is defined in the task definition

Load Balancer Integration
    - ALB connect directly with ECS tasks in EC2 Instance which is inside of ECS cluster
    - NLB is recommenced only for high throughput / high performance use cases, or to pair it with AWS private Link
    - CLB is also supported but not recommended

EFS is ideal Data Volume for both launch types

Deploying Updates
    When we are trying to update service, we can see the parameters `min running task` and `max running task`
    for example, we have 6 tasks running on the service as v1 version. We want to update from v1 to v2
    max running task is 100% and min one is 200. 
    In this case, tasks can be from  6 to 12: along with running v1 tasks, other 6 v2 tasks start running.
    So, we have 12 tasks. And the old v1 tasks are terminated. Then, all tasks are refreshed

ECS with EventBridge:
    Tasks may be created based on an event (uploading file to S3 bucket). 
    - A file is updated to S3 bucket.
    - Amazon EventBridge get the event, and with proper role EventBridge runs a new task in ECS.
    - The created task includes needed role to communicate with S3 (GetObject), and can process the updated file (or other operations (DB))

    - EventBridge may schedule tasks (run/stop)

    - EventBridge can serve as a reverse: when something happens with task/container, the EventBridge may trigger SNS and send email to admin


ECS Task Definitions
    Task Definitions are metadata in JSON form to tell ECS how to run a Docker container
    
    - Image name
        Up to 10 containers in a Task Definitions
        If a container is marked as an essential, when it fails, other containers are gonna be failed
    - Port Binding for container and host
        EC2:
            We well run docker container in EC2 inst by the definition (the instance must have ECS Agent)
            The container tie network with its port
            And also we must map instance's port to the definition as well

            When you map the container port but don't map host port, we get Dynamic Host Port Mapping
            Thanks for the Dynamic Host Port Mapping feature, ALB can easily address to ECS Task without Host port (It is only for ALB)
            We must allow on the EC2 instance's Security Group for any port from the ALB
            Dynamic Host Port gives different port for the same instance. 
            For example we want to run 2 same images in the same instance in the same port. If we define host port, host and container ports' 
                                                                               combinations will be the same and the second container fails
            When we give 0 for host port host even we are running the same container port, it differs as host ports are different 
        Fargate:
            Unlike EC2 launch type, there will be unique private IP address for each task
            here only container port is defined (because there is not any host here)
            ECS ENI Security Group to allow needed port from the ALB -> ALB Security Group to allow prt 80/443 from web
    - Memory and CPU required
    - Networking information
    - IAM Role
        We specify IAM role for a Task Definition and it is gonna be inherited for all its tasks
        For example Task_A_Definition has a role that can get obj from S3 while Task_B_Definition has a role to load data to Aurora

        there are 2 IAM role we can specify in Definition:
            Task Role: to operate with aws resources
            Task Execution Role: ECS Agent's property to operate tasks (run/stop/pull image from ECR)
    - Logging Configurations
    - Environment Variables/Files(bulk)
        Hardcoded (URLs)
        SSM Parameter Store - sensitive Variables (API keys, shared configs)
        Secret manager - sensitive Variables (DB password)
        Amazon S3 - fetching data from S3 bucket
    - Data Volumes (Bind Mounts)
        Shared data between multiple containers in the same Task Definition
        Work for both EC2 and Fargate tasks
        - ec2:
            Data is died to the lifecycle of the ec2 instance
        -Fargate:
            Data is tied to the container(s)
            20GB-200GB (default 20)

ECS Task Replacement:
    NOTE!!! It is not for Fargate. Fargate is fully managed by AWS as it is serverless

    When a task of type EC2 is launched, ECS must determine where to place it, with the constraints of CPU, Memory or available port
    Similarly, when a service scales in, ECS need to determine which task to terminate
    
    To do this, we can define a task placement strategy and task placement constraints
    ECS Task placement process order:
        1. Identify the instances that satisfy the CPU, Memory, and port requirements in the task definition
        2. Identify the instances that satisfy the task placement constraints
            - distinctinstance: place each task on a different container instance
                "placementConstraints": [
                    {
                        "type": "distinctInstance"
                    }
                ]
            - memberOf: Places task on instances that satisfy an expression (uses Cluster Query Language)
                "placementConstraints": [
                    {
                        "expression": "attribute:ecs.instance-type =~ t2.*",
                        "type": "memberOf"
                    }
                ]
        3. Identify the instances that satisfy the task placement strategy
            - Binpack: place tasks based on the least available amount of CPU or memory. minimizes the number of instance in use (cost saving)
                "placementStrategy": [
                    {
                        "field": "memory",
                        "type": "binpack"
                    }
                ]
            - Random: selects instance randomly
                "placementStrategy": [
                    {
                        "type": "random"
                    }
                ]
            - Spread: Place the task evenly based on the specific value (AZ, instance id for example). High availability
                "placementStrategy": [
                    {
                        "field": "attribute:ecs.availability-zone",
                        "type": "spread"
                    }
                ]
            - You can mix all of them:
                "placementStrategy": [
                    {
                        "field": "attribute:ecs.availability-zone",
                        "type": "spread"
                    },
                    {
                        "field": "memory",
                        "type": "binpack"
                    }
                ]
        4. Selects proper instance

ECR: we can create a repository here, and pu;;/push our docker images. It works directly with ECS (ECS have to include proper role)
Copilot: Generate .Docker script containerized application


------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------Elastic Beanstalk-----------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Main Components:
    Application - collection of envs, versions, ect...
        - Web Server Tier: traditional way that receives traffic from internet and send it to backends
        - Worker Tier: Instances receives SQS messages and process them. The instances are called workers
        - App name
        - Platform: You will choose in which platform you are working (Python, Node.js, etc)


    Beanstalk Deployment Options for Update
        - All at once (Fastest deployment. 
            All instances in v1 stop -> Deployment operations -> new v2 instances run)
            - No cost. But when deploying is going, no any instance will be available
        - Rolling
            Let's say we have 6 instances with v1 code. and we set Bucket size = 2 (2 instances serve as buckets)
            so, 2 out of 5 will stop -> deployment -> 2 as v2 and 3 as v1
            then, again 2 out of 3 v1 will stop -> deployment -> 4 as v2 and 1 as v1
            finally, remain 1 instance will be transitioned to v2
            Now, all instances go latest version of app.

            - No any extra cost, During above deployment process the app's capacity will decreases.
            - Application will run on both versions simultaneously
        - Rolling with additional batches
            similar to Rolling option, but new temp instance in number of bucket size will run
            the same example. 5 instances as v1
            2 extra instances in v2 will added -> 2 v1 apps in instances will be stops
            -> 2 v2 will be runs instead of 2 stopped apps in instances -> 2 v1 stops
            -> 2 v2 starts -> 1 v1 stops -> 1 v2 starts -> the 2 extra instances will be terminated at the end 
            after the loop 5 v2 apps in the instances will remain. 

            - It charges additional cost. Good for prod environment
        - Immutable
            Another temp ASG with a instance (v2) will be created parallel to current ASG ->
            if the instance with v2 pass health check, remain instances will also be runs ->
            The new instances v2 will be added to current ASG, and sll v1 instances will be terminated

            - quick rollback if issue, 0 downtime, high cost, double capacity, great for prod
        - Blue / Green
            Another entire (green stage) environment with v2 will be created.
            And some traffic will be routed to the stage (by Route 53) for testing purposes.
            After passing testing
            When staging is OK, we just delete v1 (blue) env and swap the URL of the envs using Beanstalk

            - This is nor direct feature, it is manually
        - Traffic Splitting
            A temp ASG will be created with the same capacity with v2. 
            -> certain % of traffic is routed to the new ASG to test. If failure automatically rollback
            if good the temp is migrated to main ASG -> The v1 app is then terminated

            - No application downtime

Lifecycle policy
    EB can store at most 1k app versions. 
    If you won't delete a version after exceeding 1k you cannot upload any version anymore
    That's why you must specify LIFECYCLE POLICY !!!

    - Based on time (period after which a version will be deleted)
    - Based on space (number after which a version will be deleted)
    !!! Current version never be deleted even it meets lifecycle policy condition

Migration:
    You cannot change LB type after creation of EB env. 
        to change, create the same env except LB manually and deploy and perform CNAME swap
    
    You can prevision RDS in EB env but it is not good option for prod env as it is depend on EB
        keep separated RDS from EB. Protect DB from deletion



------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------CloudFormation-----------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Infrastructure as Code
We just write/upload the code of infrastructure to CF and cloudfront do everything itself (creations, orders, logs)
YAML / JSON
Resource is MANDATORY thing in CF.
    use `!Ref <parameter/resource>` to get parameter or resource as a value
There are more than 200 resources in AWS, all syntaxes are available on documentation (e.g. AWS::ec2::instance)

Parameter: You can use  to input it in future (or it depends on user's/account's/etc variable)  
    `  
    Parameters:
        SecurityGroupDescription:
            Description: Security Group Description
            Type: String
    `
    using: ... GroupDescription: !Ref SecurityGroupDescription
Mapping:   It is similar to parameter but hardcoded inside the yaml script (AWS regions, etc)
    `
    RegionMap:
        us-east-1:
            "32": "ami-58495834ef2"
            "64": "ami-5748943kf6"
        us-west-1:
            ...
    `
    using: ... ImageId: !FindInMap [RegionMap, !Ref "AWS::Region", 32]

Outputs:
    In AWS CloudFormation, the "Outputs" section of a template allows you to define values that you want to be easily
    accessible after the stack is created. These outputs can be referenced by other stacks, scripts, or by users to
    obtain information about the resources created during the stack deployment.

Conditions:
    `
    Conditions:
        CreateProdResources: !Equals [!Ref EnvType, prod] (And Equals, If, Not, Or)
    `
    Conditions can be applied to resources / outputs / etc...


Functions
    - Fn::Ref
    - Fn::GetAtt (Getting an attribute of a resource. e.g: ! GetAtt EC2instance.AvailabilityZone)
    - Fn::FindInMap
    - Fn::ImportValue
    - Fn::Join (result is "a:b:c" of `!Join [:, [a, b, c]]`)
    - Fn::Sub
    - Condition Functions

What if Fail when
    creating:
        by default everything is gonna rollback (created things are deleted)
        we have options to disable rollback and troubleshoot the issue (preserving successfully provisioned resources)

    updating:
        the same conf with creating 
        Rolls back to previous state by default

To get Stack events (emIL, lambda) enable SNS Integration using Stack Options

ChangeSets
    When you update a stack, you need to know what changes before it happens for greater confidence
    Change set won't say if the update will be successful. 
    YOu do it bu creating ChangeSet from the original Stack. If you like you can update

Nested Stacks
    you can nest a stack inside another. 
    RDS stack, ASG stack or ELB stack can be nested in App Stack for example

Stack Set   
    To group multiple stacks from different account/region/etc
    if you update one of them, all are gonna be updated

    The permission will be only to Admin and trusted users


Drift
    Even CloudFormation creates resources automatically, it does not block manually change
    For example, you can edit security group after it is created by CF
    To detect such drifts, from Stack action click `detect drift` CF says if any drift detected

Stack Policy
    You can attach a policy that allow/deny actions on the resources (policy is in JSON format)
    Protecting resources from unintentional updates for example

------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------Amazon SQS-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
SQS is queuing service: Producers send the message to sqs and and messages are polled by consumers
No throughput message number limit, retention period of message is 4 days by default. maximum is 14
Low latency (<10 ms on publish and receive)
Message size limitation is 256KB
Consumer can receive up to 10 messages at a time

SQS with ASG
    best practice is assigning ASG as a producer. 
    When there are too many messages in SQS CloudWatch alarms (if metric exceeds) to scale out

SQS Access Policy
    Like bucket policy, you have to configure sqs access policy
    allow/deny specific user/account to send/produce message
    For example to get messages from S3 bucket, you have to give access on SQS Queue to receive message from S3 

Visibility Timeout
    SQS Queue uses at-least-once. There may be 2 consumer at a time but we should avoid from this process.
    Only a consumer need to consume one message.
    
    We can set Visibility Timeout. It means from the beginning of the time (when a producer receive a message)
    the message stay invisible during this period. 
    What if the period ends before consumer complete process?
    In this case, consumer send ChangeMessageVisibility API call to expend the period. And the message keeps invisibility
    Default value is 30 sec

Dead Letter Queue
    !!!Dead Letter Queue is a separated queue object. We will create another Queue for DLQ purpose

    We know that if a consumer cannot consume message message goes back to SQS Queue again
    But what if a message can't be processed for many time? The problem is with the MESSAGE
    
    To recognize such kinda "bug" messages we can set threshold of numbers how many times a message
    can go back to SQS queue. After that threshold message goes to Dead Letter Queue for debugging purposes

    after fixing codes we can apply redrive the message from DLQ Queue to source Queue.

Long Polling (Receive Message Receiving)
    We can wait for message during polling if there is no any message in queue yet
    It is useful as no need extra API calls, and no latency
    Waiting time 1-20 sec. 20 is preferable
    Can be enabled at the queue level or at the API level using ReceiveMessageWaitTimeSeconds

SQS Extended Client
    What if queue a video content that is more than 256KB? 
    Simple. save it S3 bucket->send s3 metadata to sqs -> sqs sends metadata to consumer 
                    -> by the metadata consumer takes the video just from S3 bucket

API
    CreateQueue(MessageRetentionPeriod), DeleteQueue
    PurgeQueue: delete all message in queue
    SendMessage(DelaySeconds), ReceiveMessage, DeleteMessage(MaxNumberOfMessage) -> default 1 (up to 10)
    ReceiveMessageWaitTimeSeconds: Long Polling
    ChangeMessageVisibility: visibility timeout

FIFO (First in First Out)
    receiver gets message by order
    no duplication (in per 5 minutes)
    group id and deduplication id
    
    De-duplication methods:
        Content based: do sha-256 hash the message body and de-duplicates based on this
        Deduplication ID: By explicitly assigned ID

    We can group by messages by giving IDs. It is useful for dedicated receiving, separately deduplication/throughput dedicating

------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------Amazon SNS-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------

We saw SQS that it queues messages and producers process the message
But what if there we need to send one message to many receivers (email, SQS, S3, etc)?
In this case SNS helps us. It is not Queue service but pub/sub or publish and subscribe
A producer publish something to SNS topic and subscribers get notified
All receivers get all messages (can be filtered)
up to 12.5m subscribers per topic. up to 100k topic limits
Topic Publish vs Direct Publish (both are performed by SDK)

For example multiple SQS queues should be notified about events but you can do it only a SQS in S3 bucket
    To tickle that, publish the event to SNS Topic and make multiple SQS queues as subscribers of the SNS topic

FIFO is available in SNS as well (Group ID, Deduplication ID). You kan connect FIFO SNS with FIFO SQS

Message Filtering
    can apply a filter policy for one/multiple Consumers. No filter means all message

- Standard SNS Topic publish message to 8 targets (Kinesis Firehose, SQS, Lambda, Email, Email Json, SMS, HTTP, HTTPS)
- FIFO is to only SQS Queue

!!!!!!!!NOTE: Receiver (SQS) should subscribe itself to Sender (SNS) to proper configure access policies, not reverse


------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------Kinesis--------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
- Kinesis Data Streams: capture, process, store data streams
    Well so far. now we knew about queue and pub/sub. What about stream?
    You are using mobile phone and by Kinesis DS apps analyze your sensor data in real time (continuous, ordered, sequenced, scalable)
    Retention 1-365 days
    Immutability: once data is inserted in Kineses, it can't be deleted
    data that shares the same partition id goes to the same shard (ordering)

    Capacity Mode:
        Provisioned
            Shards. can choose # of shards, scale manually/API
            You pay per shard provisioned per hour
        On-demand
            No need in advance provisioning something
            Just you use and don't care about capacity
            4MB/s and 4000 msg/sec by default
            Scales automatically based on observed throughput peak during the last 30 days
            Pay per stream per hour & data in/out per GB
    Shard
        You provision SHARDs to work with stream data. Shards are numbered. YOu provision them ahead of time
        For example you can start with 6 shards. Data from stream is gonna spread across shards
        Shards is manner of how much producer/consumer data can work with your Kinesis DS (throughput)
    Producer
        1MB/sec and 1000msg/sec per shard
        PutRecord API
        - Application
        - Client (Mobile)
        - SDK, KCL (Kinesis client library)
        - Kinesis Agent
    Consumer
        2MB/sec (shared) per shard all consumer / 2MB/sec (enhanced) per shard per consumer
        - Apps
        - Lambda
        - Kinesis Data Firehose
        - Kinesis Data Analytics
    Record
        It is a "message"
        Producers send record to NDS and KDS sends it to Consumers
        Partition key (must input records)
        Data blob (value)
        Sequence no. It defines which shard the data is coming from (unique per partition-key within shard). Only in getting message
        Distribution of producers (assigning partition key) is our job. If limit exceeds (>1MB) it raises ProvisionedThroughputExceed error
            Solutions: 
                Use highly distributed partition key
                Retries with exponential backoff
                Increase shards
    Security
        Can implement client side encryption
        VPS Endpoints available for Kineses to access within VPC
        Monitor API calls using CloudTrail

    KCL
        Java based Kineses Client Language is used to consume stream data.
        on shard can reference to only one KCL instance. 4 shard = max 4 KCL applications
        KCL can run on EC2, Beanstalk or on-premises
        Progress is checkpointed into DynamoDB (We need a DynamoDB table with right accesses in KCL)
        KCL 1.x : supports only shared consumer
        KCL 2.x : supports both shared and enhanced fan-out

    Scaling
        Shard Splitting
            Used to divide a "hot shard" into 2 new shards (multiplying the capacity) in one operation
            the old hot shard is closed and deleted when its data expired
            Everything is MANUALLY
        
        Merging Splitting
            Reverse version of Shard Splitting
            Merge two "cool shards" into one



- Kinesis Data Firehose: load data streams into AWS data stores
    Gets data from sources -> transforms by Lambda (optional) -> Load it into destinations
    No extra ETL tool is required (but can't do full ETL needs. eg no code execution or limited transformation)
    serverless, filly managed, no admins
    Pay for data going through Firehose
    Near real time (60 sec latency minimum for non full batches) / while KDS is real time (200ms)
    Minimum 1 MB of data at a time

    - Producers
        all Kineses Data Stream producers and KDS itself
        Amazon CloudWatch
        AWS IoT
    - Transformer
        Lambda Functions
    - Destinations
        AWS Destinations (S3, Redshift, OpenSearch)
        3rd-party Partner Destinations (DataDog, mongoDB)
        Custom Destinations (HTTP Endpoints)
    - Backup (all or failed data)
        S3 backup bucket

- Kinesis Data Analytics: analyze data streams with SQL or Apache Flink
    Real time analyze data (execute code in middle)
    Serverless
    SQL Application
        Producers:
            Kineses Data Stream
            Kineses Data Firehose
            (also takes data from S3 and joins to main source data)
        Destinations:
            Kineses Data Stream
            Kineses Data Firehose
    Amazon managed Service for Apache Flink
        Use Flink (Java, Scala or SQL) to process and analyze streaming data
        Resources:
            KDS
            Amazon MSK
            !!!NO Firehose


Kineses VS SQS
    Let's say we have 100 trucks and and each of them sends data and we capture it in real time
    In this case we need ORDER to analyze/ run app by trucks. Nothing we can? Look:
    SQS Fifo:
        We can use SQS queue in FIFO type. And truck id will be GRoup id. So receiver gets data by group (truck id)
    Kineses:
        We can provision fir example 5 shards. So approximately 20 trucks stream goes each shard.
    We know about capacity of two services (for example producer throughput limit in Kineses), and we should 
    decide which one is better for our case. If trucks # are dynamic we can use SQS, but when it is known in advance
    it is better to work with shards
 
- Kinesis Video Streams: capture, process, store video streams

------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------CloudWatch-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Amazon CloudWatch is a monitoring and management service provided by Amazon Web Services (AWS).
It allows users to collect and track metrics, collect and monitor log files, and set alarms.
CloudWatch provides a comprehensive set of tools for monitoring resources, applications, and services in the AWS environment.

Metrics:
    In CloudWatch, a metric is a time-ordered set of data points. Each data point represents the value of a metric at a specific time.
    Metrics can be collected from various AWS resources such as Amazon EC2 instances, Amazon RDS databases, Amazon S3 buckets, and more.
    Examples of metrics include CPU utilization, network throughput, disk I/O, and request latency.
    -By default EC2 Instance metrics have metrics "every 5 mins"
        But with detailed monitoring feature yuo can get the data "every 1 min"
    -It is useful, for example, when want scale your ASG faster
        only 10 Detailed monitoring is available in Free Tier
    !!!!!!Note EC2 RAM consumption is not pushed by default, but you can create custom metric and the instance publish the consumption
                                                                    to your metrics (by API call)

    Custom Metrics:
        Above, everything is about automating. AWS creates all metrics for you when you create a resource. But LIMITED
        For example metrics created by AWS takes limit data (CPU, Credit, Storage), but what if we want more?
        Then we can create custom metrics. You just call PutMetricData Api call from inside of resource to CloudWatch be got aware (RAM, app logs, etc)
        Metric resolutions (StorageResolution API parameter)
            - Standard: Push metric data every 1 minute
            - High Resolution: 1/5/10/30 seconds (higher cost)
                High Resolution Alarm executed every 10/30 secs
        You can push data in up to 2 weeks past or 2 hrs future            

Dimensions:
    Dimensions are additional characteristics that can be associated with a metric.
    They are name-value pairs that provide more information about the data points in a metric.
    For example, if you are monitoring the CPU utilization of EC2 instances, you might have dimensions such as
    InstanceID, AutoScalingGroupName, etc. These dimensions help you narrow down the scope of your metrics.
    Up to 30 dimensions per metric

Cloudwatch Logs
    Log groups: arbitrary name, usually representing an application
    Log Stream (in log group): instance within application / log files / containers
    Can define log expiration policies (never expire / 1 day-10 yrs)
    Logs can be sent to (Destination)
        S3 (exports)
        Kineses Data Stream
        Kineses Data Firehose
        Lambda 
        OpenSearch
    Logs encrypted by default (can setup KMS-based with own keys)

    Logs can be sent from (Source):
        SDK, CloudWatch Logs Agent, CloudWatch Unified Agent
        Elastic Beanstalk: collection of logs from application
        ECS: collection from container
        AWS lambda: collection from function logs
        VPC Frow Logs: VPC specific logs
        API Gateway
        CloudTrail based on Filter
        Route53: Log DNS queries

    Logs Insights:
        You can write a simple script to get an insight from your app's logs
        e.g: Find a specific IP inside a log, count occurrences of "ERROR" in your logs
        It provides a purpose-build query language. 
            It automatically discovers fields from AWS and JSON log events
            Fetch desired event fields, filer based on conditions, calculate aggregate statistics,
                                sort events, limit of events

            It is a query engine (not real time). You will analyze historical data only
    
    S3 Export:
        You can export logs to S3 to retire. logs take up to 12 hours to be ready for export
        API: CreateExportTask
        It is not real or even near time. So use Logs Subscription instead in case of real time

    Log Subscription
        To get real time log events from CloudWatch Logs for processing and Analysis
        Subscribers can be KDS, KDF or Lambda
        Yo can assign Subscription Filter to get only certain logs
        In addition, you can get logs from multiple regions/accounts into an account and analyze or retire it
            Sender creates Subscription Filter & Recipient has Subscription Destination (with Destination access policy)
                and IAM Role  for allowing PutRecord to KDS (can be assumed by 1st account)
    
    Metric filters:
        How can we take specific log as a Metric and analyze? By Metric Filter (in Log Group section)
        e.g we can take all logs which contains "ERROR" and save them in "ERROR" custom metric
        Metric value: if one "ERROR" is found as what the metric should accept. Usually 1
                                means: 1 error = 1.
        Looks like: EC2 -> CW Logs -> Metric Filters -> CW Alarm -> SNS
    
    EC2 Instances:
        How can cloudwatch gets log from inside of our server (ec2 or on-premises)?
        By CloudWatch Agent. we just set CloudWatch agent to our server and specify which 
                            logs we are sending. 
        For example we need to be aware of CPU utilization.
        2 types:
           CloudWatch Logs Agent
               Older version. can send to only CW Logs
            CloudWatch Unified Agent
                Collects both metrics and logs and send them to CloudWatch
                Collect additional system-based metrics (RAM, CPU, Disk)
                Collects all data in your server (instance, local linux)

CloudWatch Alarms:
    Alarms are used to trigger notification any metric
    It is created based on CloudWatch Logs Metrics Filter
    Options: max/min,%,sampling,etc
    States:
        OK - not alarm yet
        INSUFFICIENT_DATA - no enough data to alarm
        Alarm - Alarmed
    period
    Target
        EC2 - stop/terminate/reboot, recover an instance
        ASG - scale in / out
        SNS

    Composite alarms: Combination of multiple alarms (e.g stop instance if cpu is 95%+ AND ram in use of 99%+)

CloudWatch Synthetics Canary
    You just write script of actions and CloudWatch Synthetic does it behalf of a user.
    It can measure response time, availability of your URL/API and can take a screenshot of UI
    Scripts are written in Node.js or Python 
    you can integrate it with Alarm to take action 
    can run once or regularly scheduled 

    Canary recorder - record your action on a website and automatically get script
    
    Blueprints
        Heartbeat Monitor - load URL, store screenshots and an HTTP archive file
        API canary - test api functions
        Broken Link Checker - check all links in URL 
        Visual Monitoring - compare a screenshots taken during a canary wun with baseline screenshot


------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------Amazon EventBridge-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Formerly cloudWatch Events
- Triggers Lambda functions, send SQS/SNS messages
- Based on Schedule (Cron Jobs) or events (start EC2, S3 event/etc)
- React: Event rules to react toa service doing something (root user may notified when new user logs in)

Event Bus
    Default Event Bus (from AWS services such as ec2,lambda/etc)
    Partner Event Bus (from partner SaaS like DATADOG, zendesk)
    Custom Event  Bus (from custom apps)

Events can be accessed by other AWS accounts using Resource based Policies
Events can be archived and the archives can be replayed
  


------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------X-RAY-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
You are running a big app in aws includes huge amount of micro distributed services. How would you monitor them?
Of course with X-Ray. Visual Analysis of our applications
We can see and monitor our app as 5 fingers
Understanding dependencies and microservice architectures of our project
review request behavior
Identify impacted users
Services:
    Lambda
    Elastic Beanstalk
    ECS
    ELB
    API Gateway
    Ec2 instance or any apps(on-premises)
Well, we have app: Front end (web), back end & DB
TRACE and (Sub) Segments:
    When user calls HTTP request, trace begins. 
    web server creates segment in the trace (HTTP method, parameters)

    Then web server calls backend server. 
    Backend creates sub-segment in trace (precessing time, CPU utilization)
    
    And it calls DB. DB creates another sub-segment and add annotations 
    and back to back end. etc etc

    This process make of Trace

Wow!! X-RAY is magic. But how can I use it in my app?
    take these 2 actions
    - First you need to choose X-RAY SDK supporting language (Python, Node.JS, Java, GO, .NET)
        And inside app import the AWS X-RAY SDK 
        you are required some code modification to set up X-RAY
        Using this imported module, your app sends traces to X-RAY "agent" of your machine

    - X-RAY Daemon acts like "agent" here
        next step is installing the X-RAY Daemon into your machine
        or for AWS Services just enable X-RAY integration
        It just publish the traces to AWS

    Apps must have proper IAM role to load data to X-RAY

    To recap, your ec2 instance must be installed X-RAY daemon and codes must import X-RAY SDK

Sampling:
    Are you really think you should gain all requests into X-RAY. NO need.
    You can sample the incoming request to decrease the charge and resources
    You can set:
        Priority: 1-9999 (defining rule's priority)
        reservoir (max number of requests per second)                 \
        rate: the rate (%) of requests after exceeding reservoir size / -> default 1 and 5%
        You can set also Matching Criteria 

Annotations: Key values which added custom by developer. It is used for filter or additional info. It is indexed (can use search)
Metadata: NO indexed. Can't use to search or filter

APIs
    Write Policy (used by X-RAY daemon)
        - xray:PutTraceSegments: publishing segment to AWS X-RAY
        - xray:PutTelemetryRecords: upload telemetry (SegmentReceiveCount, SegmentRejectedCount, BackendConnectionError)
        - xray:GetSamplingRules: loads/update all sampling rules from AWS X-RAY
        - xray:GetSamplingTargets
        - xray:GetSamplingStatisticSummaries

    Read: 
        - xray:GetSamplingRules
        - xray:GetSamplingTargets
        - xray:GetSamplingStatisticSummaries
        - xray:BatchGetTraces: Receives a list of traces specified by ID. Each trace is a collection of segment
        - xray:GetServiceGraph: main graph
        - xray:GetTraceGraph: service graph for one or more specific trace IDs
        - xray:GetTraceSummaries: Retrieve IDs and annotations for traces available for a specific time frame using an optional filter

Beanstalk with X-RAY
    During creation of app you can enable x-Ray
    or you can enable by .config file of app so that it will get xray daemon
    And ec2 instance must have proper read policy of x-ray

ECS with X-RAY
    Daemon container:
        We can run x-ray by creating x-ray daemon container per ec2 instance
        10 instance 10 daemon container

        As coder, we first define daemon container (2000 port on udp) ->
        and we define app container and declare env variable "AWS_XRAY_ADDRESS"
                                                and value is the daemon name
        and link with the daemon container by assigning the name in "links" list
    
     Sidecar:
        You will run one X-RAY sidecar container alongside one app container
        10 container 10 X-RAY sidecar
        (Fargate supports only this pattern as it does not has any server)
        Sidecar runs side-by-side with app


------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------AWS CloudTrail-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Provides governance, compliance and audit for you AWS Account
 - Someone terminated my ec2 instance!
 - Go Cloud Trail and check logs
Logs from Console, SDK, CLI, AWS Services (IAM activities)
Can put logs from CloudTrail into CloudWatch Logs or S3
Can be applied to ALL regions (default) or a single regions

CloudTrial Events:
    Management events 
        operations that are performed on resources in your AWS like AttachRolePolicy/CreateTrial
        it is by default enabled
        it can be separated into two events READ and Write events on AWS account

    Data events (not default)
        Unlike Management one it does not log events on sources but data
        S3 object level activity: also Read and Write events
        AWS Lambda function execution activity (Invoke activity)
    
    CloudTrial Insight Events
        Cloud watch continuously analyze Management events and generate insights:
            such as inaccurate resource provisioning, hitting service limits etc
        CloudTrial Insight analyzes normal management events to create a baseline
        And then continuously analyzes write events to detect unusual patterns
        And they can be sent to S3 bucket/service console/EventBridge

Events Retention
    90 days
    Can be sent to S3 to retire further and logs, in this case, are analyzed by Athena

Further examples:
    User calls delete a table API from DynamoDB -> DB logs the API call ->
    event sent to EventBridge -> EventBridge push a message to SNS Topic 



------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------Lambda-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Lambda is serverless means you do not care about server, instead you define your code& time/event to run
"Invoke" is the most used term used in lambda functions.
"Event" is a obj format of JSON that contains all info about event (source service's data/etc)
"Context" is all about the invocation itself, function and runtime env. It is passed your function 
                        by Lambda. aws_request_id/function_name/memory_limit_in_mb/log_stream_name/etc

1-Lambda Synchronous Invocations
    `aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --region eu-west-1 response.json`

    As soon as you invoke LF, function runs and back response
    These apply sync invocation
        User Invoked: CLI, SDK, API Gateway, ALB, S3 batch, CloudFront
        Service Invoked: Cognito, Step Functions
    Errors must be handled by client/server side (retry, exp backoff, etc)

    ALB with lambda:
        Client can connect with Lambda function through ALB
        Client (Http request) -> ALB (converts to JSON) -> target group -> Lambda Function
        Multi-header value:
            When you enable this feature, HTTP header and string parameters that are sent with multiple values
            are shown as arrays within the AWS Lambda event and response object: http://example.com/path?name=foo&name=bar ->
                                                                                "queryStringParameters": {"name":["foo", "bar"]}
            You can enable it in TARGET GROUP attributes section

2-Lambda Asynchronous Invocations
    `aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --invocation-type Event --region eu-west-1 response.json`
    
    S3, SNS, CloudWatch events / EventBridge, AWS CodeCommit, AWS CodePipeline and etc
    It works like: client sends a request to Lambda function and does not wait for response immediately
    as it goes through queue. If a request fails, after one minute will be retried
    and if again error, after 2 minutes be run. 
    We can set DLQ -SNS and SQS after retries done. (DLQ is set in Lambda service)
    DLQ works for only in case of asynchronous type within Lambda
    Make sure the processing is idempotent
    In this type, we got 202 response event it fails, because we don't care about succeed or not, th 202 means the function is invoked

    CloudWatch Events / EventBridge 
        CRON or Rate EventBridge Rule: Trigger LF every 1 hour
        CodePipeline EventBridge Rule: Triggers LF when Stat Changes

        Create EventBridge rule and set the LF as destination. (make sure LF contains proper permission to receive invocation from CE)

    S3 event Notification
        When object created/removed/restored/replicated/etc the Event Notification may trigger LF if you define 
        We can trigger LF by these ways:
            S3->SQS->LF
            s3->LF (asynchronous by Event Notification)
        -If two writes are made to a single non-versioned object at the same time, it is possible that only a single event
        notification will be sent
        -If you don't want that you can enable versioning
        Make sure that resource-based of LF is configured true
        S3 notification will send all info in json format about the change

3-Event Source Mapping (invoked synchronously)
    Records need to be polled from the source
    Kinesis Data Streams/ SQS&SQS Fifo/DynamoDB Streams
    -If we configure Lambda to get data from Kinesis, inside Lambda Event Source 
        mapping will be created and and it poll data from Kinesis (e.g.)
        and get batch if data exists then the ESM is gonna invoke the Lambda Function 
        to process the batch of data

    This process can be divided into 2 types:
    1. Stream
        There will be event source mapping creates an iterator for each shard, process items in order
        We can configure weather start with new item, from the beginning or from timestamp
        the processed items are not being removed from the stream. Any data is not effected by the Lambda
        Use case:
        -Low traffic:
            use batch window to accumulate records before processing (staging)
        -High traffic:
            you can configure lambda to process multiple batches in parallel
            up to 10 batches per shard
            in-order processing is still guaranteed for each partition key
        ERROR:
            By default, if function fails, the entire batch is reprocessed until 
            the function is succeed, or the data in batch expires
            You can configure the process:
                Discard old events
                restrict the number of retries
                split the batch on error (to work around Lambda timeout issue)
                (discarded events can be routed to a destination)
    
    2. Queue
        The same with above: Event source Mapping poll data from SQS (fifo) ->
            invokes a LF with event batch
        Poll messages in Long Polling method
        Specify Batch size (max 10,000 for standard, 10 from FIFO)
        
        RECOMMENDED: Set the queue visibility timeout to 6X the timeout of LF
        
        We can use DLQ for problem messages in SQS Service (not Lambda)
        Or set destination to Lambda for failure
        
        LF scales up to the number of active messages group (in fifo)
        
        in in case of standard queue it scales up as quickly as possible
        Lambda adds 60more instances per minute to scale up
        up to 1000 batches of messages processed simultaneously

        the event source mapping might receive the same item from the queue twice
        
        Lambda deletes items from the queue after they are processed successfully

Destinations:
    This is new feature of AWS that store failed or succeed event for reprocess
    Synchronous:
        In this type you can communicate with your LF in real-time, so no need to destination
    
    Asynchronous:
        can be SQS, SNS, Lambda, EventBridge Bus. DLQ as well
        AWS recommends using Destination services instead of DLQ (both can be used at the same time)

    Event Source Mapping:
        for discarded event batches: SQS and SNS
    
    Use case:
        Destination is useful to retain error messages.
        For example we can route the failed events to another lambda function 
        and simultaneously route the success events to SQS Queue

Permissions:
    -We have to specify IAM Role to Lambda Function to get access use other services
    -We have to specify RESOURCE BASED POLICY to Lambda function to give access to other services
    Don't forget that principle does not define a resource but AWS identity (service, user, group or role)

Environment Variable:
    Of course Lambda creates an environment to run the code. 
    And we can assign variables in scale of env.
    - To do so go to configuration -> env. variable and assign variables
    - To get the values in the code, import the `os` and call `os.getenv("<var>")`
    - You can change variable and value without deploying an update
    - You can encrypt (SDK) the variable to keep it secret

Monitoring
    We can see the logs (prints too) and metrics in Lambda console and CloudWatch logs
    Make sure your Lambda function has proper access to log into cloudWatch

    Tracing with X-RAY
        Just enable X-RAY from configuration to run daemon for you->
        Use X-RAY SDK in your code (for customize) -> make sure lambda has access to write in X-RAY
        you can use 3 X-RAY env variable:
            _X_AMZN_TRACE_ID: contains the tracing header
            AWS_XRAY_DAEMON_ADDRESS: the X-RAY Daemon (value is ip:port)
            AWS_XRAY_CONTEXT_MISSING: by default, LOG_ERROR

Edge
    Some application execute smt at the Edge Location
    Edge function is a code you write and attach to CloudFront distribution
    It is run close to the user to minimize latency
    Use cases: You may want to route request intelligently route across origin and data centers
        Website Security and privacy
        Dynamic Web Application
        Search Engine Optimization (SEO)
        Bot Mitigation at the Edge
        Real-time Image Transformation
        User authentication and authorization, etc, etc 
    In this case we have two serverless compute options: 
    1. CloudFront Function
        Lightweight functions written in JavaScript
        For high scale, latency-sensitive CDN customizations
        less ms startup times, millions of request/second
        Used to to change Viewer request and responses (Not origin)

        for example you can deny or allow user according to its token in edge at this point

    2. Lambda@Edge
        But in Lambda@Edge you can customize  origin request/response too
        Lambda function written in NodeJS or Python
        Scales up to thousands of requests/seconds

Networking
    By default Lambda functions is launched outside of your own VPC (in AWS VPC)
    So it cannot access resources in your VPC (RDS, ElastiCache, Internal ELB, etc)
    But it available for Internet and other services in AWS VPC

    Then you can define your own VPC ID (and Subnet, security id) for lambda
    So that you can interact with your sources in the VPC (DB, Storage, ELB etc)
    Check your Lambda role first (should involve AWSLambdaVPCAccessExecutionRole)
    When you define the VPC in Lambda, ENI (Elastic Network Interface) in subnet
    so Lambda function interact with the subnet resources through the ENI
    make sure that the services security group must access the Lambda function

    YES, I got that I must create Lambda function in own VPC to communicate with my VPC 
    sources but in this case how can I access to outside (internet)?
    To do so in case of EC2 we create the instance in private subnet, 
    BUT in case of Lambda functions it does NOT work

    SO what should I do?
    Creating the LF in private subnet gives an internet access if you have a NAT Gateway / Instance
    
    Let's see process:
        Communicate with internet:
            Lambda function in private network connects with NAT in public subnet ->
            NAT sends traffic to INternet through Internet Gateway which located the same public subnet

        Communicate with AWS Services:
            first way is the same with above (through NAT -> IGW) (public connections)
            second way is establish VPC Endpoint so Lambda communicate with AWS like DynamoDB
            directly through VPC Endpoint (this is private connection)
    
    !!!!NOTE: Even you create LF in private subnet CloudWatch still works without any NAT or Endpoint

Performance Configuring
    RAM is configured starting from default value of 128 MB up to 10GB in 1MB increment
    The more you add, the more vCPU credits you get
    at 1729MB, a function has the equivalent of one full vCPU
    in case of more that 1 vCPU consider using multi-threading in your code to benefit from it
    
    Timeout: from 1 to 900 seconds (which means after the period LF doesn't process)

    Execution Context:
        temporary runtime environment that initializes any external dependencies
        Use case: DB connections, HTTP clients, SDK clients
        
        Let's say you want to connect with DB. Are you thinking defining the connection 
            inside function? BAD!!! If you do so, every invocation repeat the same thing.
            Instead you can define the connection part of code out of function. It mountains
            the connection in environment for some time. it is called EXECUTION CONTEXT.
        
        the next execution can re-use the context to execution time and save time in initializing
        connection objects

        Execution Context includes ephemeral storage or /tmp directory (up to 10 GB). 
        It is good to share. Objects are shareable across instances but not functions.
        To encrypt objects in /tmp, generate KMS Data Keys

        If you need permanent file system, use S3

Lambda Layers
    - Custom Runtime such as runtime for C++ or Rust
    - Externalize Dependencies to re-use them:
        Dependencies such as libraries or modules doesn't change often as app packages
        In this case we can externalize the dependencies from the app's packages.
        It can be reused by other functions as well.
        Just create a layer, upload dependency and import it in your code

Storage in lambda
    We have 4 Storage options:
        /tmp         : max 10 GB
        lambda layers: 5 layers per function up to 250 MB total
        S3           : Elastic
        EFS          : Elastic           

    EFS
        You can enable EFS if the VPC is enabled in lambda
        The EFS objects is shared across Lambda Functions
        path/
        Must leverage EFS Access Points

Lambda Concurrency
    concurrency = (average request per second)*(average duration of requests in seconds)
    -> when a function invoked, a environment created and init and lambda handle codes run
    -> during this process there is other invocation, a new env is created and init and handler runs
    -> if both request take average 10 seconds, concurrency = 10*2 = 20

    Reserved Concurrency
        1 region in 1 account can have 1000 concurrency limit
        So be careful to distribute 1000 limit across all functions
        We can specify the number of concurrency limit of instance of a function up to 1000
        To do so set a reserved concurrency at function level
        we can get more than 1k limit by opening a support key
    
    Each invocation over the limit wil trigger a "Throttle"
    Throttle behaviors:
        if synchronous  => return ThrottleError - 429
        if asynchronous => retry and if again so then go to DLQ
            For the Throttle errors(429) and system errors(5xx), Lambda 
                returns the event to the queue and attempts to run the function again
                for up to 6 hours
                
                the retry interval increases exponentially from 1 second after 1st attempt 
                to a maximum of 5 minutes

    Cold Start:
        If you have heavy initial process (out of function), first instance 
            start slow because of initialization (DB connection/etc)
            this is called cold start

    Provisioned Concurrency:
        Allows you to set an initial number of concurrent executions for a function to ensure
        that the function has sufficient capacity to handle incoming requests.

        to avoid Cold Start you can run the first instance in advance that before user do it
        In this case cold start never happens
        Application Auto Scaling can manage provisioning concurrency (schedule or target utilization)

------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------DynamoDB------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------

each row is up to 400KB
Data types:
    Scalar         - String, Number, binary, Boolean, Null
    Document Types - List, Map
    Set types      - String Set, Number set, Binary list

Primary Key
    It is mandatory to choose a PK for a table during creation
    PK can be in two options:
        Partition Key (or HASH key)
            Should be high cardinality
            Unique user id for example
        Partition Key + Sort Key
            For example a table which records games.
            User id can be partition key, a user can attend more than 2 games
            In this case Game id field will be Sort key.

Table's capacity
    Provisioned
        You specify the number of reads/writes per second              \
        You need to plan capacity beforehand                            |
        Pay for provisioned capacity units                              |
                                                                        | - You can switch diff modes once in 24 hrs
    On-demand                                                           |
        Read/write automatically scale up/down with your workloads      |
        No capacity planning need                                       |
        Pay for what you use, more expensive (2.5x Provisioned)        /

    Write Capacity Units (WCU):
        1 WCU represents 1 write per second for an item up to 1KB
        If the items are larger than 1KB, more WCU are consumed
        
        Sample 1: 20 items are written in a table per second and each item size is 2KB
                  20 * (1 sec / 1 sec) * ceil(2KB) = 40 WCUs
        Sample 2: 120 items per minute, each item 4.5 KB
                  120 * (1 sec / 60 sec) * ceil(4.5KB) = 2 * 5 = 10
    
    Write Capacity Units (RCU):
        When data is written into DB, in backend, it is done to a certain server
        And this server replicates the dato to other read services
        and client is addressed to the other service to read data from.
        REGARDING this process read capacity is divided into 2 types
        
        - Eventually Consistent Read (default)
            It is possible some data staling when requesting just after it is uploaded
            After, for example some milliseconds
        
        - Strongly Consistent Read
            No any data stale.
            Now upload, then you can read it now.
            To apply this option in GetItem, BatchGetItem, Query or Scan APIs, we set `ConsistentRead` as True
            it charges double RCUs of Eventually Consistent Read
        
        Calculating RCU
            1 RCU represents 1 Strongly Consistent Read or 2 Eventually Consistent Read per sec, for an item up to 4KB
            if a item more than 4 KB more RCU will be added
            Sample 1:
                10 Strongly Consistent Read per second, 4KB item size
                10 * 1sec * ceil(4KB/4KB) = 10RCUs
            
            Sample 2:
                10 Eventually Consistent Reads per second with 6KB size:
                (10/2) * 1 sec * ceil(6KB/4KB) = 5*2 = 10 RCUs
                It would be 20 if Strongly Consistent Read option was enabled

Partitions Internally
    Data is stored in partitions
    Partition Keys go through a hashing algorithm to know to which partition they go to
    To compute number of partitions:
        - by capacity = (RCU/3000)+(WCU/1000)
        - by size = TotalSize/10 GB
        - #of partitions = ceil(max(# by capacity, # by size))
    WCUs and RCUs are spread evenly across partitions

Throttling:
    If we exceed provisioned RCU or WCU, we get "ProvisionedThroughputExceededException"
    Reasons:
        How Keys/Partitions - a partition is requested too often
        Very large Items - RCU and WCU depend on size of items
    Solution:
        Exponential backoff (already in SDK)
        Distribute partition keys as much as possible
        If RCU issue, we can use DynamoDB Accelerator (DAX)
