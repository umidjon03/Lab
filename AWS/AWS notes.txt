----------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------IAM - CLI-----------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
3 ways to access AWS
- via console
- CLI by access keys
- SDK by access keys

Access keys are generated through the AWS console and managed by a User

IAM security tools:
- IAM credentials Report (account-level)
- IAM Access Advisor

AWS CLI Profile:
    In CLI how can we deal with multiple accounts? --profile.
    Usually, we log in to an account by `$aws configure`. And in this case we log in to the account as default
    But we can log in as separated account by `$aws configure --profile < profile name you want>`
    And we can use the new account in this way: `aws s3 ks --profile <the name>`

MFA in CLI
    To log in via MFA we need to create a STS temporary session by MFA credentials. And every logging in we use the session token (129 - lesson)
    `aws sts get-session-token --seral-number <it is available in user credentials> --token-code <generated by your MFA device>`

--------------------------------------Cost Management--------------------------------------------------
in root account activate "IAM user/role access to billing information" to monitor billing in an IAM user

- create budget to alert when a target exceeds

------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------EC2------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------
!!! I am gonna back to types of instance and purchase of instance soon, lesson 131, 132,133, Event bus, registry code

EBS - is like USB storage but not physical, it is locked to an Availability Zone
- It uses network to communicate the instance, which means there might be a bit of latency
- It can be detached from an EC2 instance and attached to another one quickly
- An EBS volume in us-east-1a cannot be attached to us-east-1b
- To move a volume across, you first need to snapshot it
- You can take a snapshot from a EBS without detaching, but not doing it is recommended

EBS Snapshot Features:
- EBS Snapshot Archive: make your snapshot archive tier that is 75% cheaper
- But takes 24-72 hours to be restoring the archive
- Recycling Bin for EBS snapshot: you can specify a retention period for a snapshot (1 day - 1 year)
- Fast Snapshot Restore: Force full initialization of snapshot to have no latency on the first use by more CASH $$$

EBS types
EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec)

- gp2/gp3: general purpose SSD volume that balance price and performance                                
    - Cost effective and low latency
    - System boot volumes, Virtual desktops
    - 1GB - 16TB
    - gp3 is newer generation of gp2
    - in gp2 volume and IOPS are linked while in gp3 they are independent
    - means: 3 IOPD per GB, at 5334 GB we are at the max IOPS (16000) in gp2 (baseline: 3000 IOPS)
    - in gp3 baseline IOPS 3000 and throughput 125 MB/s,  can increase up to 16000 IOPS and 1000MB/s tp independently

- Provisioned IOPS or PIOPS:
- io1/io2: highest-performance SSD volume for mission-critical low-latency or high-throughput workloads | used as boot volumes
    - Critical business application with sustained IOPS performance |
    - Or applications that need more than 16000 IOPS                | - 4 Gb-16 TB. max: 64k PIOPS for Nitro EC2 & 32k for others
    - Independently                                                 | 
- io2 Block Express 4GB to 64 TB: very fast max: 256k PIOPS with an IOPS:GB ratio of 1000:1

- st1: low cost HDD volume designed for frequently accessed, throughput-intensive workloads
- sc1: lowest cost HDD volume designed for less frequently accessed workloads
- Supports EBS Multi-attach

EBS Multi Attach:
- Up to 16 Ec2 Ins can have a volume (only io1/io2)
- It looks like several applications are connected to a general EBS
- All instances and volume should be in the same AZ

AIM
- You can take a image by customizing an instance
- or you can buy it from Amazon marketplace
- By creating Image you can create a new instance from it

EC2 Instance store
- as an alternative to Volumes, you can leverage EC2 instance store. It is a peace of the instance not another part
- Better I/O performance
- But the data is gonna be destroyed when the instance is stopped
- Good for buffer / cache/ scratch data / temp content
- Risk of data loss if hardware fails
- Backups and Replications are your responsibility


EC2 Metadata:
    - ec2 instance can see easily see its metadata by several API requests. By just 2 step:
    - TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"` (specifying TOKEN)
    - curl -H "X-aws-ec2-metadata-token: $TOKEN" -v http://169.254.169.254/latest/meta-data/{metadata path}
    - e.g. if IAM role is associated to your ec2 instance you can get the token by requesting: 
            curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/identity-credentials/ec2/info/security-credentials/



##############################################################
Load Balancer
- There are 3 types of load Balancer: Classic (out of date), Application, Network, Gateway Load Balancers.
    - ALB: layer 7 - receives HTTP/HTTPS protocols and get them to target addresses (instances, IPs etc)
    - NLB: layer 4 - receives TCP/UDP protocols and get them to target addresses (instances, IP, ALBs, etc)
    - GLB: receives requests and get them 3rd party system, if everything is good, the requests are sent to main Applications (target addresses)

- Sticky Session (Session Affinity) - the same client is redirected to the same server even by load balancer to take advantage from cookies, etc
                                                                                      !!! to configure stickiness edit Target Group attributes!!!
        - Application-based Cookies
            - Custom Cookies:        created by application for its own optimization
            - Application Cookies:    created bt LB
        
        - Duration-based Cookies: Generated by LB

When using an Application Load Balancer to distribute traffic to your EC2 instances, the IP address you'll receive
requests from will be the ALB's private IP addresses. To get the client's IP address, ALB adds an additional header
called X-Forwarded-For contains the client's IP address.

You can configure the Auto Scaling Group to determine the EC2 instances' health based on Application Load Balancer
Health Checks instead of EC2 Status Checks (default). When an EC2 instance fails the ALB Health Checks, its marked
unhealthy and will be terminated while the ASG launches a new EC2 instance.

There's no CloudWatch Metric for "requests per minute" for backend-to-database connections. You need to create a
CloudWatch Custom Metric, then create a CloudWatch Alarm.

For each Auto Scaling Group, there's a Cooldown Period after each scaling activity. In this period, the ASG doesn't
launch or terminate EC2 instances. This gives time to metrics to stabilize. The default value for the Cooldown
Period is 300 seconds (5 minutes).

##############################################################
Cross-Zone Load Balancer
- If this feature is enabled, load balancer will balance the traffic among all targets regardless of the AZ
- If disabled, all AZs get the same amount of traffic and its (AZ's) servers take equally the traffic which is directed to AZ
- It is by default enabled in ALB and does not charge, while disabled in NLB and GLB by default, and charges if enabled
- You can configure it in Target Group level 

##############################################################
Connection Draining (Deregistration Delay)
- Let's say our instance is out of service due to in-flight requests or taking scaling processes, in this case the ALB/NLB identifies the instance is unhealthy
    So stops sending new requests to it. And waits for the time which specified (by default 300 seconds) 0 to 3600 can be chosen

-----------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------EFS------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------
EFS is an elastic file system which is connected via Network (Network Fie System) with
- It can be associated with EC2 instance when creation
- Whe you are creating a EC2 ins, by default, other security groups are gonna be created by system and associated 
to the EFS and EC2 (instance-sg-1 > instance | efs-sg-1 > efs) and they maintain the connection
- you can find an outbound rule in the sg which addresses to the EFS server
- you cannot connect with your EFS without the rule (port is 2049)





-------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------RDS------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------

RDS is Relational Database Service of AWS. It can involve Oracle DB, SQL Server, MYSQL, Postgres, Aurora
- Primary DB is used for all transactions and connected with application

READ REPLICA
- You can have up to 15 Read replicas in RDS (15 for Aurora as well)
- Read Replica is used for only read operations (analyze, BI)
- Data is replicated from Primary to Read Replica in ASYNC method
- Unlike other services, RDS does not charges for inter AZ activities. 
- But you will be charged for Cross Regional activities

MULTI AZ (Disaster Recover)
- It is backup database to which data is replicated SYNC to immediately reflect data 
- It is good when one one AZ is bombed your data is available in other AZ
- !!! A Read Replica can also set up as a Multi AZ RDS
- Primary and Multi AZ RDBs have the same domain so that there is no need any action to tie your app with new backup DB when primary is bombed 


!! Oracle does not support IAM database authentication 

------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------ElastiCache--------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------
Like RDS you can enable multi AZ for ElastiCache as well

        METHODS:

1. Lazy Loading / Cache Aside / Lazy Population
    - Cache hit: If app get data from cache it is ok. it is called cache hit
    - Cache miss: If add can't get data from cache due to absence
    - Read from DB: after above case, the app get data from DB
    - Write to cache: And white the data to the cache

2. Write-Through
    - If some data is added or updated the it will be also done in Cache
    - You can take more advantage if integrate this method with the Lazy load
    - TTL is not good choice with this method

Cache Eviction methods
1. Explicitly
2. Evicting by LRU (least Recently Used) when a metric reach a top
3. Time-to-Live (TTL). Setting a period in which data is gonna be retained in the cache
(If too many evictions piss off you, you can scale up or out your ElastiCache)

5 - the maximum number of Read Replicas you can add in an ElastiCache Redis Cluster with Cluster-Mode Disabled?

????????????????????????????
A source DB instance can have cross-Region read replicas in multiple AWS Regions
A Read Replica in a different AWS Region than the source database can be used as a standby database and promoted
to become the new production database in case of a regional disruption. So, we'll have a highly available (because
of Multi-AZ) RDS DB Instance in the destination AWS Region with both read and write available.
????????????????????????????


----------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------Route 53---------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------
When we go 

Alias names don't route to EC2 instances
ALIAS does not charge
We cannot use root domain / APEX in CNAME. But can it in ALIAS
    for example: we have a domain: umidjon.com, and it should route to ALB. CNAME cannot use umidjon.com as a domain but can app.umidjon.com
    But the with ALIAS, we can assign a ELB or CloudFront, etc to the root domain

Record Types:
    - A:         Maps a domain or subdomain to IPv4
    - AAAAAA:    Maps a domain or subdomain to IPv4
    - CNAME:     Routes to another domain
    - ALIAS:     Routes to a AWS resource  (for A and AAAAAA record types)

DNS cache:
    - TTL. We can specify TTL in which client remember that certain domain maps to certain IP
    - ALIAS: we cant specify explicitly TTL for ALIAS

Routing Policies:
    - Simple: 
        - typically specify a record. But can include multiple targets (IPs), if so client choose a record randomly.
        - If ALIAS, it returns only a aws resource
        - Can't be attached Health check

    - Weight:
        - you will distribute traffics among records by setting weights to records. 1-255. 
        - Record names and types must be the same.
        - Can be attached Health check
    
    - Latency Based
        - Redirects the client to the resource that has the least latency
        - Latency is based on traffic between users and AWS Region
        - It is good when latency is crucial
        - Can be associated with health check
    
    - Failover
        - There are 2 types of records: primary and secondary
            - Primary: mandatory to specify Health Checks
            - secondary: optional to specify Health Checks. It is Failover record in case of primary fails
    
    - Geolocation
        - You associate a resource with a Continent / Country / US States
        - Can associate HCH
        - Must specify a resource as a default
    
    - Geoproximity (using Route 53 Traffic Flow feature)
        - Route traffic to resources based on the geographic location of users and resources
        - Unlike Geolocation policy, this has ability to shift more traffic to resource based on defined bias
        - bias can be 1 to 99 and -1 to -99 to expand or shrink traffic to resource

    - IP based
        - Routes based on client's IP (CIDR range) to specific resource which you specified
    
    - Multi-Value Answer
        - Returns multi healthy IPs/resources 

Health Check
    - We can prevent our clients from being redirected to unhealthy resource (mainly Public)
    - To do that, during creation of record, we need to associate a Health Check
    - Health Checks are integrated with CW metrics
    - Multiple HCHs the all around world can check the endpoint
        - HTTPS/HTTP/TPC
        - if > 18% of health checkers report the endpoint is healthy, the service consider it is healthy, otherwise unhealthy
        - you can set parameters healthy/unhealthy threshold. It says about status after a certain number (which you set) of consecutive checks 
    - your resource is OK if HCH returns 2xx or 3xx status
    - configure your router/firewall to allow incoming request from Route 53 Health Checkers
    - you can combine child HCHs (AND, OR, NOT) to make decision of Parent HCH
    - To set health checkers for the private resources, first you need to attach CW to the resource and tie the HCH with the CW Alarms


    Types of Health Checks:
        - HCH that monitors an endpoint
        - HCH that monitors other Health Checks
        - HCH that monitors Cloud Watch Alarm

-----------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------VPC------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------------
VPC: provides network isolation, allowing you to create a private, dedicated portion of the cloud for your resources.
    This isolation is crucial for security and privacy.

Subnets: Within a VPC, you can create subnets to further organize and segment your resources. Subnets are associated
    with specific availability zones within a region.

    Subnets can be divided into 2 types:
    - Public: Directly goes to internet by internet gateway
    - Private: Cannot communicate with external internet. Only way to communicate internet is that
       You will create NAT gateway / instance in public Subnet, and it directs the private subnet to the internet via Internet Gateway

SG (Security Group) & NACL (Network Access Control Lists):
    - NACL is subnet level. Include allow & block list. only ip can be specified. Stateless
    - SG is instance level. Include only allow list. Can contain IP and other SG. Stateful

    Traffic first come to NACL, if passed come to SG.
    All instance in a subnet are set NACL of the subnet automatically

VPC Flow Logs
    Every traffic log to a VPC is recorded to VPC Flow Log
    Logs of VPC, Subnet or Elastic Network Interface (ENI)
    Helps to monitor / troubleshoot connectivity issues. Debugging
    The logs can be sent to S3, CloudWatch Logs, and Kinesis data Firehose
    Captures network information from AWS managed interfaces too: ELB, ElastiCache, RDS, Aurora, etc

VPC Peering
    To connect resources between VPSs we need explicitly assign VPC Peering between the two VPCs which we are connecting
    Must not have overlapping CIDR. 

VPC Endpoints
    Usually all services in AWS are publicly, but what if a service is private?
    Let's say we have a EC2 instance in private submit and how can we connect to it on behalf a random AWS Service?
        - via VPC Endpoint Gateway   - for S3 and DynamoDB
        - via VPC Endpoint Interface - the rest
    In that case you will connect to the EC2 privately, securely, within the VPC, without latency. 
        That is why you need to use VPC endpoints instead of connecting from outside (www) via NET Gateway (etc)

Connecting to VPC
    - Site to site VPN
        - It provides  connection btw your office (for example) and VPC by public connection (www)
        - Connection is encrypted automatically

    - Direct Connect (DX)
        - Establishing physical connectivity btw on-premises and aws
        - It is totally private, secure and fast
        - Goes through a private network
        - Takes at least a months to establishing
    





------------------------------------------------------------------------------------------------
=================================IP/CIDR========================================================
------------------------------------------------------------------------------------------------
Public IP is provided by your internet service provider.
Local IP is provided by your router. By the local IP you can connect with co-networkers easily (you can play game)
IPv4 is from 0.0.0.0 to 255.255.255.255 in human eyes. But why 255? because 255 is maximum value of 8-bit binary
In fact, the range is from 00000000.00000000.00000000.00000000 to 11111111.11111111.11111111.11111111

what is CIDR tho? CIDR is a protocol defines in which range the router (or other provider) give you and co-networkers IPs
for example: 192.132.01.0/24 means that your IP provider can give all users in range of from 192.132.01.0 to 192.132.01.255
/24 means the first 24-bit remains the same, but remain 8 bits can be changed.
if we define the IP as binary: 11000000.10000100.00000001.00000000
                               \________24-bit__________/ \_8-bit_/
your router keeps the 24 bit the same and remain 8 bits are can be changed dynamically




------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------S3------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Features and Performance
    Buckets are region level but its name must be globally unique. S3 itself is global service
    bucket.folder1/subfolder1/object.pdf
        \_________________/       \_/
               prefix           suffix
    max size of object can be 5TB.
    suitable at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket
    no prefix limits
    Multi-Part upload recommended for 100MB+ and must be used for 5GB+ files (it optimizes upload by parallelism)
    Byte-Range Fetches do almost the same functionality with Multi-Part but for download. divides into bytes and requests to local machine parallel
        Can be used to retrieve only partial data very fast
    S3 Transfer Acceleration increases upload speed by using Edge Location as a middleman
    S3 Select and Glacier Select filters data within the s3 and retrieve it to the app:
        Regular apps: {Extract} => Filter => Load or {Extract} => Load => Filter. in this case your app goes down
        SQL Select and Glacier Select: {Extract&Filter} => Load . up to 400% faster and 80% cheaper
    Presigned URL. In Object action you can generate presigned url for an object
    Access Points. You can create AP like bucket policy. It allows user to use specific resource in the bucket

Security
    - We can set bucket or object policy to control access into the bucket/object
    - Can be assigned policy by IAM policy
    - Encryption can be enabled
    - Block Public Access can be enabled

We can enable static website hosting in directory level
We can enable versioning in bucket level

Replication: (Go bucket's Management section)
    - Must enable versioning in source and destination buckets
    - Cross-Region Replication (CRR)
    - Same-Region Replication (SRR) 
    - Buckets can be in different AWS account
    - Copying is asynchronous
    - Must give proper IAM permissions to S3
    - After enabling replication, only new objs are replicated
        - to replicate existing objs, use S3 Batch Replication
    - Deletes are not replicated
        - delete markers can be replicated if explicitly enabled
        - but still permanent deletes is not be applied

S3 Storage Classes: (By life cycle you can automate changing btw storage classes based on a certain condition)
    - Amazon S3 Standard                        : 99.99 availability, for frequently accessed data, low latency and high through. UC: Big data, game apps
    - Amazon S3 Standard-infrequent Access (AI) : 99.99 availability, Lower cost, for infrequently accessed data. UC: backup, Disaster Recovery
    - Amazon S3 One Zone-infrequent Access      : High durability (99.9999999999%) in single AZ. 99.5 availability. data lost when AZ is bombed. UC: second backup
            
            Glacier Storages: Low cost of data (archival). Price for storage+retrieving
    - Amazon S3 Glacier Instant Retrieval       : Milliseconds retrieval, minimum storage duration is 90 days. UC: data accessed once a quarter
    - Amazon S3 Glacier Flexible Retrieval
        - Expedited                             : 1 to 5 mins retrieval     \
        - Standard                              : 3 to 5 hrs retrieval      |  -> min storage duration 90 days
        - Bulk                                  : 5 to 12 hours (free)      /   
    - Amazon S3 Glacier Deep Archive
        - Standard                              : 12 hours retrieval        \
        - Bulk                                  : 48 hours retrieval        /  -> min storage duration 180 days 

    - Amazon S3 Intelligent Tiering             : small monthly monitoring and auto-tiering fee. No retrieval charges, based on usage objects are tiered automatically
        - Frequently access tier                    : default
        - Infrequent access tier                    : not accessed for 30 days
        - Archive Instant access tier               : not accessed for 90 days
        - Archive Access tier (optional)            : configurable from 90 to 700+ days
        - Deep Archive Access tier (optional)        : configurable from 180 to 700+ days


S3 Lifecycle Rules: (Go bucket's Management section)
    - Transitions Actions: configure objects to transition to another storage class
        e.g. you can set storage class to transit automatically after 60 inactive days
    - Expiration actions: configure objects to expire (delete) after some time
       - for log files (can be set 265 days for example)
       - old version of files 
       - incomplete multi-part uploads
       - we can assign it by prefix or tag

S3 Analytics - Storage Class Analysis
    - Help you decide when to transition objects to the right storage class
    - Provide recommendations for Standard and Standard IA
    - Does NOT work for One-Zone or Glacier
    - Report is updated daily

S3 Event Notification (Configure it in Properties section of a Bucket)
    This service sends notification about changes on S3 to SNS, SQS or Lambda Functions (CreateObject, RemoveObject, etc)
    Instead of attaching a role to S3, we have to attach needed policies to that resources
    Can filter objects
    you can use Amazon EventBridge for event purposes, broader service while Event Notification is for only s3.

User-defined Object Metadata and S3 Object Tags
    - You can define a metadata for a object. But it must start with "x-amz-meta-" as key-pairs
    - You can also assign tags to classify your objects
    - you cannot search objects by its metadata or tags. instead, you can use an external DB as a search index such as DynamoDB

Object Encryption (header: "x-amz-server-side-encryption":"<value>")
    
    With Amazon S3 Managed Keys - SSE-S3 (Default)  <value> = AES256                                             \
    With KMS Keys stored in AWS KMS - SSE-KMS       <value> = aws:kms                                            | -> Server-Side Encryption (SSE)
    With Customer-Provided Keys - SSE-C             Key must be provided in headers (HTTPS is mandatory)         /
    Client Side Encryption                          Client encrypts file and upload it
    
    You can specify a logic that says in S3 policy: 'If transit is not secure deny it' - enforces user to use HTTPS
    You can also deny traffic based on your encryption requirements

S3 CORS. Cross-Origin Resource Sharing
    In web browser if a site send request to another page/resource web browser ensures if
    CORS is enabled for the other site by sending request. For example, in web browser open
    'inspection' of a page and write in console: fetch('google.com'). This means your origin
    is requesting another origin. If CORS is enabled you can successfully send get request. 

    ORIGIN = schema (protocol) + host + port
    For example you request a html file from a web server and as a response the server reference another cross-origin 
        server. If CORS is enabled, the referenced server gives you a html without any problem
    CORS headers must be involved preflight response of the second server. If the client satisfy the headers, it send a http (e.g) request to the server
    In Amazon S3, if client makes a cross-origin request on a S3 bucket, we need to enable the correct CORS headers
    In S3, go to CORS section and configure the json like:
        [
            {
                "AllowedHeaders": [
                    "Authorization"
                ],
                "AllowedMethods": [
                    "GET"
                ],
                "AllowedOrigins": [
                    "<url of first bucket with http://...without slash at the end>"
                ],
                "ExposeHeaders": [],
                "MaxAgeSeconds": 3000
            }
        ]

MFA  Delete
    We can enforce users leveraging MFA to do important operations on S3. For permanent delete and suspending versioning
    To use MFA Delete, Versioning must be enabled on the bucket
    if you want to enable MFA Delete, disable all life cycle rules
    Only root account can enable/disable MFA Delete
    # enable mfa delete
        `$aws s3api put-bucket-versioning --bucket mfa-demo-stephan --versioning-configuration Status=Enabled,
                            MFADelete=Enabled --mfa "arn-of-mfa-device mfa-code" --profile root-mfa-delete-demo`
    After enabling MFA Delete, important actions are done by MFA devices via CLI

Access Log
    We can enable access log on a bucket
    every activity is gonna be logged into the destination bucket
    Do NOT choose current bucket as a destination, else you waste credits by infinity log
    Your destination bucket's policy will be changed. 



------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------CloudFront-------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Also Content Delivery Network (CDN)
Improves read performance by caching at the edge
For example: You have a S3 (any backend origin). When you enable CloudFront Distribution on the S3 address,
    client first looks up cache from the nearest Edge Location. If not found, Cloud Front bring the page to the Edge 
    from the S3 and it is cached during TTL. And it will available for other users
For this, you must create CloudFront distribution (Origin Access Control too) and configure S3's bucket policy to access on an origin
When you call a distribution:
    - it says to Cache Behaviors: "Hey this is url, route this url to needed Origin (backend)" \
    - Cache Behaviors takes the url and routes to specified Origin                             | -> /login -> ec2 (login backend), /* -> s3 (default)
    - And needed origin starts working.                                                        /

Advanced Concepts:
    - CloudFront Edge locations are all around the world
    - The cost of data out per edge location varies 
    - To reduce cost, reduce Edge locations. There are 3 classes:
        - All: all regions are available. Best performance
        - 200: Most regions except for most expensive ones
        - 100: Only the least expensive regions
    - Origin Groups: 
        - to increase high-availability and do failover
        - origin group: one primary and one secondary origin
        - If the primary origin fails, the secondary one is used

Cache Policy
    Contents are cached with their unique identifier. With this UID edge location retrieves exact content from cache.
        Default key is hostname+resource portion
        But you can add other elements (HTTP headers, cookies, query strings) to the cache key using CloudFront Cache Policies
            - HTTP header                   None, Whitelist
            - Cookies                       None, Whitelist, Include All-Except, All
            - query string (after ? mark)   None, Whitelist, Include All-Except, All

Origin Request Policy
    Specify values that you want to include in origin requests without including them in the Cache Key 
    For example, header incudes Secret Key but you don't want to ad it to the Cache key
    You can include: 
        - HTTP header                   None, Whitelist, All viewer headers option
        - Cookies                       None, Whitelist, All
        - query string (after ? mark)   None, Whitelist, All
    Ability to distinguish CloudFront headers and Custom headers (which is not related to caching operations)
    Instead create a custom policy, you can use Predefined managed Policies

CloudFront Invalidations
    Well. You set TTL to refresh Cache. But what is the underlying reference is changed before TTL?
    In that case we can enforce an entire (*) or partial cache (/images/*) to refresh
    This section invalidates specified caches immediately when base is changed

To maximize performance, separate dynamic and static origins

ALB and EC2 as an origin
    ALB must be public and open to edge locations' traffic. In this case EC2 can be private
    EC2 instances must be public and open to edge locations' traffic

Geographic Restrictions
    You can list of countries from which traffic should be allowed/blocked
    It uses third-party service to identify countries's ips

Signed URL / Signed Cookies
    Some files or cookies can be exist that you do not want to be them publicly. Only specific user can access them
    In this case use signed url/cookies. 
    For this, you need to attach a policy with:
        includes URL expiration
        Includes IP ranges to access them from
        Trusted signers (which AWS accounts can create signed URLs)
    
    Signed URL     = access to individual files
    Signed Cookies = access to multiple files

    Signed url uses RSA key to encrypt. Field level Encryption, 
        - Edge location encrypts data by public key, and it is decrypted by private key in last step (EC2 instance) 


------------------------------------------------------------------------------------------------
===============================Encryption & Decryption==========================================
------------------------------------------------------------------------------------------------

Encryption is changing the body of a data to plaint text (ciphertext) (by a E(*arg) function). Decryption is reverse of Encryption (a D(*arg))
let's say we have 2 people Alice and Bob.

Symmetric Encryption:
    Alice sends a message to Bob by E(m)=c encryption. To read Bob decrypts the ciphertext by D(d)=m

Asymmetric Encryption:
    Alice and Bob have a common key. So alice encrypts a message like E(m, key)=c. Bob decrypts like D(c, key)

RSA:
    RSA is asymmetric encryption algorithm that generates public and private keys.
    Public keys can be shared but private must not be done!
    Alice and Bob both have public and private keys:
    Alice to send a message to Bob. Alice encrypts message by E(m, <public key of Bob>) = c
    Bob decrypts it by D(c, <his own private key>).
    
    Without private key you can never decrypt ciphertext.


------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------ECS--------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------

ECS is Elastic Container Service to run Docker containers in AWS
Launch Docker containers on AWS = Launch ECS Tasks on ECS Cluster

EC2 Launch Type:
    you must provision & maintain the infrastructure (EC2 instance)
    There is a ECS Cluster and it contains EC2 Instances. In case of ECS, each instance must run the ECS Agent to register in the ECS Cluster
    Then AWS takes care of starting/stopping containers in EC2 instances
    In ECS, security group's rules does NOT matter

Fargate Launch Type:
    Launch docker containers on AWS too.
    You do not provision the infrastructure (no EC2 instance)
    It's all serverless
    We just create task definitions in ECS Cluster and AWS runs the task for us based on te CPU/RAm you need
      
IAM roles for ECS Tasks
    EC2 Instance Profile:
        Used by the ECS Agent
        Makes API calls to ECS service
        Send container logs to CloudWatch Logs
        Pull docker image from ECR
    
    ECS Task Role:
        Attached roles for each Task in EC2 instance
        Task role is defined in the task definition

Load Balancer Integration
    - ALB connect directly with ECS tasks in EC2 Instance which is inside of ECS cluster
    - NLB is recommenced only for high throughput / high performance use cases, or to pair it with AWS private Link
    - CLB is also supported but not recommended

EFS is ideal Data Volume for both launch types

Deploying Updates
    When we are trying to update service, we can see the parameters `min running task` and `max running task`
    for example, we have 6 tasks running on the service as v1 version. We want to update from v1 to v2
    max running task is 100% and min one is 200. 
    In this case, tasks can be from  6 to 12: along with running v1 tasks, other 6 v2 tasks start running.
    So, we have 12 tasks. And the old v1 tasks are terminated. Then, all tasks are refreshed

ECS with EventBridge:
    Tasks may be created based on an event (uploading file to S3 bucket). 
    - A file is updated to S3 bucket.
    - Amazon EventBridge get the event, and with proper role EventBridge runs a new task in ECS.
    - The created task includes needed role to communicate with S3 (GetObject), and can process the updated file (or other operations (DB))

    - EventBridge may schedule tasks (run/stop)

    - EventBridge can serve as a reverse: when something happens with task/container, the EventBridge may trigger SNS and send email to admin


ECS Task Definitions
    Task Definitions are metadata in JSON form to tell ECS how to run a Docker container
    
    - Image name
        Up to 10 containers in a Task Definitions
        If a container is marked as an essential, when it fails, other containers are gonna be failed
    - Port Binding for container and host
        EC2:
            We well run docker container in EC2 inst by the definition (the instance must have ECS Agent)
            The container tie network with its port
            And also we must map instance's port to the definition as well

            When you map the container port but don't map host port, we get Dynamic Host Port Mapping
            Thanks for the Dynamic Host Port Mapping feature, ALB can easily address to ECS Task without Host port (It is only for ALB)
            We must allow on the EC2 instance's Security Group for any port from the ALB
            Dynamic Host Port gives different port for the same instance. 
            For example we want to run 2 same images in the same instance in the same port. If we define host port, host and container ports' 
                                                                               combinations will be the same and the second container fails
            When we give 0 for host port host even we are running the same container port, it differs as host ports are different 
        Fargate:
            Unlike EC2 launch type, there will be unique private IP address for each task
            here only container port is defined (because there is not any host here)
            ECS ENI Security Group to allow needed port from the ALB -> ALB Security Group to allow prt 80/443 from web
    - Memory and CPU required
    - Networking information
    - IAM Role
        We specify IAM role for a Task Definition and it is gonna be inherited for all its tasks
        For example Task_A_Definition has a role that can get obj from S3 while Task_B_Definition has a role to load data to Aurora

        there are 2 IAM role we can specify in Definition:
            Task Role: to operate with aws resources
            Task Execution Role: ECS Agent's property to operate tasks (run/stop/pull image from ECR)
    - Logging Configurations
    - Environment Variables/Files(bulk)
        Hardcoded (URLs)
        SSM Parameter Store - sensitive Variables (API keys, shared configs)
        Secret manager - sensitive Variables (DB password)
        Amazon S3 - fetching data from S3 bucket
    - Data Volumes (Bind Mounts)
        Shared data between multiple containers in the same Task Definition
        Work for both EC2 and Fargate tasks
        - ec2:
            Data is died to the lifecycle of the ec2 instance
        -Fargate:
            Data is tied to the container(s)
            20GB-200GB (default 20)

ECS Task Replacement:
    NOTE!!! It is not for Fargate. Fargate is fully managed by AWS as it is serverless

    When a task of type EC2 is launched, ECS must determine where to place it, with the constraints of CPU, Memory or available port
    Similarly, when a service scales in, ECS need to determine which task to terminate
    
    To do this, we can define a task placement strategy and task placement constraints
    ECS Task placement process order:
        1. Identify the instances that satisfy the CPU, Memory, and port requirements in the task definition
        2. Identify the instances that satisfy the task placement constraints
            - distinctinstance: place each task on a different container instance
                "placementConstraints": [
                    {
                        "type": "distinctInstance"
                    }
                ]
            - memberOf: Places task on instances that satisfy an expression (uses Cluster Query Language)
                "placementConstraints": [
                    {
                        "expression": "attribute:ecs.instance-type =~ t2.*",
                        "type": "memberOf"
                    }
                ]
        3. Identify the instances that satisfy the task placement strategy
            - Binpack: place tasks based on the least available amount of CPU or memory. minimizes the number of instance in use (cost saving)
                "placementStrategy": [
                    {
                        "field": "memory",
                        "type": "binpack"
                    }
                ]
            - Random: selects instance randomly
                "placementStrategy": [
                    {
                        "type": "random"
                    }
                ]
            - Spread: Place the task evenly based on the specific value (AZ, instance id for example). High availability
                "placementStrategy": [
                    {
                        "field": "attribute:ecs.availability-zone",
                        "type": "spread"
                    }
                ]
            - You can mix all of them:
                "placementStrategy": [
                    {
                        "field": "attribute:ecs.availability-zone",
                        "type": "spread"
                    },
                    {
                        "field": "memory",
                        "type": "binpack"
                    }
                ]
        4. Selects proper instance

ECR: we can create a repository here, and pu;;/push our docker images. It works directly with ECS (ECS have to include proper role)
Copilot: Generate .Docker script containerized application


------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------Elastic Beanstalk-----------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Main Components:
    Application - collection of envs, versions, ect...
        - Web Server Tier: traditional way that receives traffic from internet and send it to backends
        - Worker Tier: Instances receives SQS messages and process them. The instances are called workers
        - App name
        - Platform: You will choose in which platform you are working (Python, Node.js, etc)


    Beanstalk Deployment Options for Update
        - All at once (Fastest deployment. 
            All instances in v1 stop -> Deployment operations -> new v2 instances run)
            - No cost. But when deploying is going, no any instance will be available
        - Rolling
            Let's say we have 6 instances with v1 code. and we set Bucket size = 2 (2 instances serve as buckets)
            so, 2 out of 5 will stop -> deployment -> 2 as v2 and 3 as v1
            then, again 2 out of 3 v1 will stop -> deployment -> 4 as v2 and 1 as v1
            finally, remain 1 instance will be transitioned to v2
            Now, all instances go latest version of app.

            - No any extra cost, During above deployment process the app's capacity will decreases.
            - Application will run on both versions simultaneously
        - Rolling with additional batches
            similar to Rolling option, but new temp instance in number of bucket size will run
            the same example. 5 instances as v1
            2 extra instances in v2 will added -> 2 v1 apps in instances will be stops
            -> 2 v2 will be runs instead of 2 stopped apps in instances -> 2 v1 stops
            -> 2 v2 starts -> 1 v1 stops -> 1 v2 starts -> the 2 extra instances will be terminated at the end 
            after the loop 5 v2 apps in the instances will remain. 

            - It charges additional cost. Good for prod environment
        - Immutable
            Another temp ASG with a instance (v2) will be created parallel to current ASG ->
            if the instance with v2 pass health check, remain instances will also be runs ->
            The new instances v2 will be added to current ASG, and sll v1 instances will be terminated

            - quick rollback if issue, 0 downtime, high cost, double capacity, great for prod
        - Blue / Green
            Another entire (green stage) environment with v2 will be created.
            And some traffic will be routed to the stage (by Route 53) for testing purposes.
            After passing testing
            When staging is OK, we just delete v1 (blue) env and swap the URL of the envs using Beanstalk

            - This is nor direct feature, it is manually
        - Traffic Splitting
            A temp ASG will be created with the same capacity with v2. 
            -> certain % of traffic is routed to the new ASG to test. If failure automatically rollback
            if good the temp is migrated to main ASG -> The v1 app is then terminated

            - No application downtime

Lifecycle policy
    EB can store at most 1k app versions. 
    If you won't delete a version after exceeding 1k you cannot upload any version anymore
    That's why you must specify LIFECYCLE POLICY !!!

    - Based on time (period after which a version will be deleted)
    - Based on space (number after which a version will be deleted)
    !!! Current version never be deleted even it meets lifecycle policy condition

Migration:
    You cannot change LB type after creation of EB env. 
        to change, create the same env except LB manually and deploy and perform CNAME swap
    
    You can prevision RDS in EB env but it is not good option for prod env as it is depend on EB
        keep separated RDS from EB. Protect DB from deletion



------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------CloudFormation-----------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Infrastructure as Code
We just write/upload the code of infrastructure to CF and cloudfront do everything itself (creations, orders, logs)
YAML / JSON
Resource is MANDATORY thing in CF.
    use `!Ref <parameter/resource>` to get parameter or resource as a value
There are more than 200 resources in AWS, all syntaxes are available on documentation (e.g. AWS::ec2::instance)

Parameter: You can use  to input it in future (or it depends on user's/account's/etc variable)  
    `  
    Parameters:
        SecurityGroupDescription:
            Description: Security Group Description
            Type: String
    `
    using: ... GroupDescription: !Ref SecurityGroupDescription
Mapping:   It is similar to parameter but hardcoded inside the yaml script (AWS regions, etc)
    `
    RegionMap:
        us-east-1:
            "32": "ami-58495834ef2"
            "64": "ami-5748943kf6"
        us-west-1:
            ...
    `
    using: ... ImageId: !FindInMap [RegionMap, !Ref "AWS::Region", 32]

Outputs:
    In AWS CloudFormation, the "Outputs" section of a template allows you to define values that you want to be easily
    accessible after the stack is created. These outputs can be referenced by other stacks, scripts, or by users to
    obtain information about the resources created during the stack deployment.

Conditions:
    `
    Conditions:
        CreateProdResources: !Equals [!Ref EnvType, prod] (And Equals, If, Not, Or)
    `
    Conditions can be applied to resources / outputs / etc...


Functions
    - Fn::Ref
    - Fn::GetAtt (Getting an attribute of a resource. e.g: ! GetAtt EC2instance.AvailabilityZone)
    - Fn::FindInMap
    - Fn::ImportValue
    - Fn::Join (result is "a:b:c" of `!Join [:, [a, b, c]]`)
    - Fn::Sub
    - Condition Functions

What if Fail when
    creating:
        by default everything is gonna rollback (created things are deleted)
        we have options to disable rollback and troubleshoot the issue (preserving successfully provisioned resources)

    updating:
        the same conf with creating 
        Rolls back to previous state by default

To get Stack events (emIL, lambda) enable SNS Integration using Stack Options

ChangeSets
    When you update a stack, you need to know what changes before it happens for greater confidence
    Change set won't say if the update will be successful. 
    YOu do it bu creating ChangeSet from the original Stack. If you like you can update

Nested Stacks
    you can nest a stack inside another. 
    RDS stack, ASG stack or ELB stack can be nested in App Stack for example

Stack Set   
    To group multiple stacks from different account/region/etc
    if you update one of them, all are gonna be updated

    The permission will be only to Admin and trusted users


Drift
    Even CloudFormation creates resources automatically, it does not block manually change
    For example, you can edit security group after it is created by CF
    To detect such drifts, from Stack action click `detect drift` CF says if any drift detected

Stack Policy
    You can attach a policy that allow/deny actions on the resources (policy is in JSON format)
    Protecting resources from unintentional updates for example

------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------Amazon SQS-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
SQS is queuing service: Producers send the message to sqs and and messages are polled by consumers
No throughput message number limit, retention period of message is 4 days by default. maximum is 14
Low latency (<10 ms on publish and receive)
Message size limitation is 256KB
Consumer can receive up to 10 messages at a time

SQS with ASG
    best practice is assigning ASG as a producer. 
    When there are too many messages in SQS CloudWatch alarms (if metric exceeds) to scale out

SQS Access Policy
    Like bucket policy, you have to configure sqs access policy
    allow/deny specific user/account to send/produce message
    For example to get messages from S3 bucket, you have to give access on SQS Queue to receive message from S3 

Visibility Timeout
    SQS Queue uses at-least-once. There may be 2 consumer at a time but we should avoid from this process.
    Only a consumer need to consume one message.
    
    We can set Visibility Timeout. It means from the beginning of the time (when a producer receive a message)
    the message stay invisible during this period. 
    What if the period ends before consumer complete process?
    In this case, consumer send ChangeMessageVisibility API call to expend the period. And the message keeps invisibility
    Default value is 30 sec

Dead Letter Queue
    !!!Dead Letter Queue is a separated queue object. We will create another Queue for DLQ purpose

    We know that if a consumer cannot consume message message goes back to SQS Queue again
    But what if a message can't be processed for many time? The problem is with the MESSAGE
    
    To recognize such kinda "bug" messages we can set threshold of numbers how many times a message
    can go back to SQS queue. After that threshold message goes to Dead Letter Queue for debugging purposes

    after fixing codes we can apply redrive the message from DLQ Queue to source Queue.

Long Polling (Receive Message Receiving)
    We can wait for message during polling if there is no any message in queue yet
    It is useful as no need extra API calls, and no latency
    Waiting time 1-20 sec. 20 is preferable
    Can be enabled at the queue level or at the API level using ReceiveMessageWaitTimeSeconds

SQS Extended Client
    What if queue a video content that is more than 256KB? 
    Simple. save it S3 bucket->send s3 metadata to sqs -> sqs sends metadata to consumer 
                    -> by the metadata consumer takes the video just from S3 bucket

API
    CreateQueue(MessageRetentionPeriod), DeleteQueue
    PurgeQueue: delete all message in queue
    SendMessage(DelaySeconds), ReceiveMessage, DeleteMessage(MaxNumberOfMessage) -> default 1 (up to 10)
    ReceiveMessageWaitTimeSeconds: Long Polling
    ChangeMessageVisibility: visibility timeout

FIFO (First in First Out)
    receiver gets message by order
    no duplication (in per 5 minutes)
    group id and deduplication id
    
    De-duplication methods:
        Content based: do sha-256 hash the message body and de-duplicates based on this
        Deduplication ID: By explicitly assigned ID

    We can group by messages by giving IDs. It is useful for dedicated receiving, separately deduplication/throughput dedicating

------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------Amazon SNS-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------

We saw SQS that it queues messages and producers process the message
But what if there we need to send one message to many receivers (email, SQS, S3, etc)?
In this case SNS helps us. It is not Queue service but pub/sub or publish and subscribe
A producer publish something to SNS topic and subscribers get notified
All receivers get all messages (can be filtered)
up to 12.5m subscribers per topic. up to 100k topic limits
Topic Publish vs Direct Publish (both are performed by SDK)

For example multiple SQS queues should be notified about events but you can do it only a SQS in S3 bucket
    To tickle that, publish the event to SNS Topic and make multiple SQS queues as subscribers of the SNS topic

FIFO is available in SNS as well (Group ID, Deduplication ID). You kan connect FIFO SNS with FIFO SQS

Message Filtering
    can apply a filter policy for one/multiple Consumers. No filter means all message

- Standard SNS Topic publish message to 8 targets (Kinesis Firehose, SQS, Lambda, Email, Email Json, SMS, HTTP, HTTPS)
- FIFO is to only SQS Queue

!!!!!!!!NOTE: Receiver (SQS) should subscribe itself to Sender (SNS) to proper configure access policies, not reverse


------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------Kinesis--------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
- Kinesis Data Streams: capture, process, store data streams
    Well so far. now we knew about queue and pub/sub. What about stream?
    You are using mobile phone and by Kinesis DS apps analyze your sensor data in real time (continuous, ordered, sequenced, scalable)
    Retention 1-365 days
    Immutability: once data is inserted in Kineses, it can't be deleted
    data that shares the same partition id goes to the same shard (ordering)

    Capacity Mode:
        Provisioned
            Shards. can choose # of shards, scale manually/API
            You pay per shard provisioned per hour
        On-demand
            No need in advance provisioning something
            Just you use and don't care about capacity
            4MB/s and 4000 msg/sec by default
            Scales automatically based on observed throughput peak during the last 30 days
            Pay per stream per hour & data in/out per GB
    Shard
        You provision SHARDs to work with stream data. Shards are numbered. YOu provision them ahead of time
        For example you can start with 6 shards. Data from stream is gonna spread across shards
        Shards is manner of how much producer/consumer data can work with your Kinesis DS (throughput)
    Producer
        1MB/sec and 1000msg/sec per shard
        PutRecord API
        - Application
        - Client (Mobile)
        - SDK, KCL (Kinesis client library)
        - Kinesis Agent
    Consumer
        2MB/sec (shared) per shard all consumer / 2MB/sec (enhanced) per shard per consumer
        - Apps
        - Lambda
        - Kinesis Data Firehose
        - Kinesis Data Analytics
    Record
        It is a "message"
        Producers send record to NDS and KDS sends it to Consumers
        Partition key (must input records)
        Data blob (value)
        Sequence no. It defines which shard the data is coming from (unique per partition-key within shard). Only in getting message
        Distribution of producers (assigning partition key) is our job. If limit exceeds (>1MB) it raises ProvisionedThroughputExceed error
            Solutions: 
                Use highly distributed partition key
                Retries with exponential backoff
                Increase shards
    Security
        Can implement client side encryption
        VPS Endpoints available for Kineses to access within VPC
        Monitor API calls using CloudTrail

    KCL
        Java based Kineses Client Language is used to consume stream data.
        on shard can reference to only one KCL instance. 4 shard = max 4 KCL applications
        KCL can run on EC2, Beanstalk or on-premises
        Progress is checkpointed into DynamoDB (We need a DynamoDB table with right accesses in KCL)
        KCL 1.x : supports only shared consumer
        KCL 2.x : supports both shared and enhanced fan-out

    Scaling
        Shard Splitting
            Used to divide a "hot shard" into 2 new shards (multiplying the capacity) in one operation
            the old hot shard is closed and deleted when its data expired
            Everything is MANUALLY
        
        Merging Splitting
            Reverse version of Shard Splitting
            Merge two "cool shards" into one



- Kinesis Data Firehose: load data streams into AWS data stores
    Gets data from sources -> transforms by Lambda (optional) -> Load it into destinations
    No extra ETL tool is required (but can't do full ETL needs. eg no code execution or limited transformation)
    serverless, filly managed, no admins
    Pay for data going through Firehose
    Near real time (60 sec latency minimum for non full batches) / while KDS is real time (200ms)
    Minimum 1 MB of data at a time

    - Producers
        all Kineses Data Stream producers and KDS itself
        Amazon CloudWatch
        AWS IoT
    - Transformer
        Lambda Functions
    - Destinations
        AWS Destinations (S3, Redshift, OpenSearch)
        3rd-party Partner Destinations (DataDog, mongoDB)
        Custom Destinations (HTTP Endpoints)
    - Backup (all or failed data)
        S3 backup bucket

- Kinesis Data Analytics: analyze data streams with SQL or Apache Flink
    Real time analyze data (execute code in middle)
    Serverless
    SQL Application
        Producers:
            Kineses Data Stream
            Kineses Data Firehose
            (also takes data from S3 and joins to main source data)
        Destinations:
            Kineses Data Stream
            Kineses Data Firehose
    Amazon managed Service for Apache Flink
        Use Flink (Java, Scala or SQL) to process and analyze streaming data
        Resources:
            KDS
            Amazon MSK
            !!!NO Firehose


Kineses VS SQS
    Let's say we have 100 trucks and and each of them sends data and we capture it in real time
    In this case we need ORDER to analyze/ run app by trucks. Nothing we can? Look:
    SQS Fifo:
        We can use SQS queue in FIFO type. And truck id will be GRoup id. So receiver gets data by group (truck id)
    Kineses:
        We can provision fir example 5 shards. So approximately 20 trucks stream goes each shard.
    We know about capacity of two services (for example producer throughput limit in Kineses), and we should 
    decide which one is better for our case. If trucks # are dynamic we can use SQS, but when it is known in advance
    it is better to work with shards
 
- Kinesis Video Streams: capture, process, store video streams

------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------CloudWatch-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Amazon CloudWatch is a monitoring and management service provided by Amazon Web Services (AWS).
It allows users to collect and track metrics, collect and monitor log files, and set alarms.
CloudWatch provides a comprehensive set of tools for monitoring resources, applications, and services in the AWS environment.

Metrics:
    In CloudWatch, a metric is a time-ordered set of data points. Each data point represents the value of a metric at a specific time.
    Metrics can be collected from various AWS resources such as Amazon EC2 instances, Amazon RDS databases, Amazon S3 buckets, and more.
    Examples of metrics include CPU utilization, network throughput, disk I/O, and request latency.
    -By default EC2 Instance metrics have metrics "every 5 mins"
        But with detailed monitoring feature yuo can get the data "every 1 min"
    -It is useful, for example, when want scale your ASG faster
        only 10 Detailed monitoring is available in Free Tier
    !!!!!!Note EC2 RAM consumption is not pushed by default, but you can create custom metric and the instance publish the consumption
                                                                    to your metrics (by API call)

    Custom Metrics:
        Above, everything is about automating. AWS creates all metrics for you when you create a resource. But LIMITED
        For example metrics created by AWS takes limit data (CPU, Credit, Storage), but what if we want more?
        Then we can create custom metrics. You just call PutMetricData Api call from inside of resource to CloudWatch be got aware (RAM, app logs, etc)
        Metric resolutions (StorageResolution API parameter)
            - Standard: Push metric data every 1 minute
            - High Resolution: 1/5/10/30 seconds (higher cost)
                High Resolution Alarm executed every 10/30 secs
        You can push data in up to 2 weeks past or 2 hrs future            

Dimensions:
    Dimensions are additional characteristics that can be associated with a metric.
    They are name-value pairs that provide more information about the data points in a metric.
    For example, if you are monitoring the CPU utilization of EC2 instances, you might have dimensions such as
    InstanceID, AutoScalingGroupName, etc. These dimensions help you narrow down the scope of your metrics.
    Up to 30 dimensions per metric

Cloudwatch Logs
    Log groups: arbitrary name, usually representing an application
    Log Stream (in log group): instance within application / log files / containers
    Can define log expiration policies (never expire / 1 day-10 yrs)
    Logs can be sent to (Destination)
        S3 (exports)
        Kineses Data Stream
        Kineses Data Firehose
        Lambda 
        OpenSearch
    Logs encrypted by default (can setup KMS-based with own keys)

    Logs can be sent from (Source):
        SDK, CloudWatch Logs Agent, CloudWatch Unified Agent
        Elastic Beanstalk: collection of logs from application
        ECS: collection from container
        AWS lambda: collection from function logs
        VPC Frow Logs: VPC specific logs
        API Gateway
        CloudTrail based on Filter
        Route53: Log DNS queries

    Logs Insights:
        You can write a simple script to get an insight from your app's logs
        e.g: Find a specific IP inside a log, count occurrences of "ERROR" in your logs
        It provides a purpose-build query language. 
            It automatically discovers fields from AWS and JSON log events
            Fetch desired event fields, filer based on conditions, calculate aggregate statistics,
                                sort events, limit of events

            It is a query engine (not real time). You will analyze historical data only
    
    S3 Export:
        You can export logs to S3 to retire. logs take up to 12 hours to be ready for export
        API: CreateExportTask
        It is not real or even near time. So use Logs Subscription instead in case of real time

    Log Subscription
        To get real time log events from CloudWatch Logs for processing and Analysis
        Subscribers can be KDS, KDF or Lambda
        Yo can assign Subscription Filter to get only certain logs
        In addition, you can get logs from multiple regions/accounts into an account and analyze or retire it
            Sender creates Subscription Filter & Recipient has Subscription Destination (with Destination access policy)
                and IAM Role  for allowing PutRecord to KDS (can be assumed by 1st account)
    
    Metric filters:
        How can we take specific log as a Metric and analyze? By Metric Filter (in Log Group section)
        e.g we can take all logs which contains "ERROR" and save them in "ERROR" custom metric
        Metric value: if one "ERROR" is found as what the metric should accept. Usually 1
                                means: 1 error = 1.
        Looks like: EC2 -> CW Logs -> Metric Filters -> CW Alarm -> SNS
    
    EC2 Instances:
        How can cloudwatch gets log from inside of our server (ec2 or on-premises)?
        By CloudWatch Agent. we just set CloudWatch agent to our server and specify which 
                            logs we are sending. 
        For example we need to be aware of CPU utilization.
        2 types:
           CloudWatch Logs Agent
               Older version. can send to only CW Logs
            CloudWatch Unified Agent
                Collects both metrics and logs and send them to CloudWatch
                Collect additional system-based metrics (RAM, CPU, Disk)
                Collects all data in your server (instance, local linux)

CloudWatch Alarms:
    Alarms are used to trigger notification any metric
    It is created based on CloudWatch Logs Metrics Filter
    Options: max/min,%,sampling,etc
    States:
        OK - not alarm yet
        INSUFFICIENT_DATA - no enough data to alarm
        Alarm - Alarmed
    period
    Target
        EC2 - stop/terminate/reboot, recover an instance
        ASG - scale in / out
        SNS

    Composite alarms: Combination of multiple alarms (e.g stop instance if cpu is 95%+ AND ram in use of 99%+)

CloudWatch Synthetics Canary
    You just write script of actions and CloudWatch Synthetic does it behalf of a user.
    It can measure response time, availability of your URL/API and can take a screenshot of UI
    Scripts are written in Node.js or Python 
    you can integrate it with Alarm to take action 
    can run once or regularly scheduled 

    Canary recorder - record your action on a website and automatically get script
    
    Blueprints
        Heartbeat Monitor - load URL, store screenshots and an HTTP archive file
        API canary - test api functions
        Broken Link Checker - check all links in URL 
        Visual Monitoring - compare a screenshots taken during a canary wun with baseline screenshot


------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------Amazon EventBridge-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Formerly cloudWatch Events
- Triggers Lambda functions, send SQS/SNS messages
- Based on Schedule (Cron Jobs) or events (start EC2, S3 event/etc)
- React: Event rules to react toa service doing something (root user may notified when new user logs in)

Event Bus
    Default Event Bus (from AWS services such as ec2,lambda/etc)
    Partner Event Bus (from partner SaaS like DATADOG, zendesk)
    Custom Event  Bus (from custom apps)

Events can be accessed by other AWS accounts using Resource based Policies
Events can be archived and the archives can be replayed
  


------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------X-RAY-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
You are running a big app in aws includes huge amount of micro distributed services. How would you monitor them?
Of course with X-Ray. Visual Analysis of our applications
We can see and monitor our app as 5 fingers
Understanding dependencies and microservice architectures of our project
review request behavior
Identify impacted users
Services:
    Lambda
    Elastic Beanstalk
    ECS
    ELB
    API Gateway
    Ec2 instance or any apps(on-premises)
Well, we have app: Front end (web), back end & DB
TRACE and (Sub) Segments:
    When user calls HTTP request, trace begins. 
    web server creates segment in the trace (HTTP method, parameters)

    Then web server calls backend server. 
    Backend creates sub-segment in trace (precessing time, CPU utilization)
    
    And it calls DB. DB creates another sub-segment and add annotations 
    and back to back end. etc etc

    This process make of Trace

Wow!! X-RAY is magic. But how can I use it in my app?
    take these 2 actions
    - First you need to choose X-RAY SDK supporting language (Python, Node.JS, Java, GO, .NET)
        And inside app import the AWS X-RAY SDK 
        you are required some code modification to set up X-RAY
        Using this imported module, your app sends traces to X-RAY "agent" of your machine

    - X-RAY Daemon acts like "agent" here
        next step is installing the X-RAY Daemon into your machine
        or for AWS Services just enable X-RAY integration
        It just publish the traces to AWS

    Apps must have proper IAM role to load data to X-RAY

    To recap, your ec2 instance must be installed X-RAY daemon and codes must import X-RAY SDK

Sampling:
    Are you really think you should gain all requests into X-RAY. NO need.
    You can sample the incoming request to decrease the charge and resources
    You can set:
        Priority: 1-9999 (defining rule's priority)
        reservoir (max number of requests per second)                 \
        rate: the rate (%) of requests after exceeding reservoir size / -> default 1 and 5%
        You can set also Matching Criteria 

Annotations: Key values which added custom by developer. It is used for filter or additional info. It is indexed (can use search)
Metadata: NO indexed. Can't use to search or filter

APIs
    Write Policy (used by X-RAY daemon)
        - xray:PutTraceSegments: publishing segment to AWS X-RAY
        - xray:PutTelemetryRecords: upload telemetry (SegmentReceiveCount, SegmentRejectedCount, BackendConnectionError)
        - xray:GetSamplingRules: loads/update all sampling rules from AWS X-RAY
        - xray:GetSamplingTargets
        - xray:GetSamplingStatisticSummaries

    Read: 
        - xray:GetSamplingRules
        - xray:GetSamplingTargets
        - xray:GetSamplingStatisticSummaries
        - xray:BatchGetTraces: Receives a list of traces specified by ID. Each trace is a collection of segment
        - xray:GetServiceGraph: main graph
        - xray:GetTraceGraph: service graph for one or more specific trace IDs
        - xray:GetTraceSummaries: Retrieve IDs and annotations for traces available for a specific time frame using an optional filter

Beanstalk with X-RAY
    During creation of app you can enable x-Ray
    or you can enable by .config file of app so that it will get xray daemon
    And ec2 instance must have proper read policy of x-ray

ECS with X-RAY
    Daemon container:
        We can run x-ray by creating x-ray daemon container per ec2 instance
        10 instance 10 daemon container

        As coder, we first define daemon container (2000 port on udp) ->
        and we define app container and declare env variable "AWS_XRAY_ADDRESS"
                                                and value is the daemon name
        and link with the daemon container by assigning the name in "links" list
    
     Sidecar:
        You will run one X-RAY sidecar container alongside one app container
        10 container 10 X-RAY sidecar
        (Fargate supports only this pattern as it does not has any server)
        Sidecar runs side-by-side with app


------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------AWS CloudTrail-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Provides governance, compliance and audit for you AWS Account
 - Someone terminated my ec2 instance!
 - Go Cloud Trail and check logs
Logs from Console, SDK, CLI, AWS Services (IAM activities)
Can put logs from CloudTrail into CloudWatch Logs or S3
Can be applied to ALL regions (default) or a single regions

CloudTrial Events:
    Management events 
        operations that are performed on resources in your AWS like AttachRolePolicy/CreateTrial
        it is by default enabled
        it can be separated into two events READ and Write events on AWS account

    Data events (not default)
        Unlike Management one it does not log events on sources but data
        S3 object level activity: also Read and Write events
        AWS Lambda function execution activity (Invoke activity)
    
    CloudTrial Insight Events
        Cloud watch continuously analyze Management events and generate insights:
            such as inaccurate resource provisioning, hitting service limits etc
        CloudTrial Insight analyzes normal management events to create a baseline
        And then continuously analyzes write events to detect unusual patterns
        And they can be sent to S3 bucket/service console/EventBridge

Events Retention
    90 days
    Can be sent to S3 to retire further and logs, in this case, are analyzed by Athena

Further examples:
    User calls delete a table API from DynamoDB -> DB logs the API call ->
    event sent to EventBridge -> EventBridge push a message to SNS Topic 



------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------Lambda-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Lambda is serverless means you do not care about server, instead you define your code& time/event to run
"Invoke" is the most used term used in lambda functions.
"Event" is a obj format of JSON that contains all info about event (source service's data/etc)
"Context" is all about the invocation itself, function and runtime env. It is passed your function 
                        by Lambda. aws_request_id/function_name/memory_limit_in_mb/log_stream_name/etc

1-Lambda Synchronous Invocations
    `aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --region eu-west-1 response.json`

    As soon as you invoke LF, function runs and back response
    These apply sync invocation
        User Invoked: CLI, SDK, API Gateway, ALB, S3 batch, CloudFront
        Service Invoked: Cognito, Step Functions
    Errors must be handled by client/server side (retry, exp backoff, etc)

    ALB with lambda:
        Client can connect with Lambda function through ALB
        Client (Http request) -> ALB (converts to JSON) -> target group -> Lambda Function
        Multi-header value:
            When you enable this feature, HTTP header and string parameters that are sent with multiple values
            are shown as arrays within the AWS Lambda event and response object: http://example.com/path?name=foo&name=bar ->
                                                                                "queryStringParameters": {"name":["foo", "bar"]}
            You can enable it in TARGET GROUP attributes section

2-Lambda Asynchronous Invocations
    `aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --invocation-type Event --region eu-west-1 response.json`
    
    S3, SNS, CloudWatch events / EventBridge, AWS CodeCommit, AWS CodePipeline and etc
    It works like: client sends a request to Lambda function and does not wait for response immediately
    as it goes through queue. If a request fails, after one minute will be retried
    and if again error, after 2 minutes be run. 
    We can set DLQ -SNS and SQS after retries done. (DLQ is set in Lambda service)
    DLQ works for only in case of asynchronous type within Lambda
    Make sure the processing is idempotent
    In this type, we got 202 response event it fails, because we don't care about succeed or not, th 202 means the function is invoked

    CloudWatch Events / EventBridge 
        CRON or Rate EventBridge Rule: Trigger LF every 1 hour
        CodePipeline EventBridge Rule: Triggers LF when Stat Changes

        Create EventBridge rule and set the LF as destination. (make sure LF contains proper permission to receive invocation from CE)

    S3 event Notification
        When object created/removed/restored/replicated/etc the Event Notification may trigger LF if you define 
        We can trigger LF by these ways:
            S3->SQS->LF
            s3->LF (asynchronous by Event Notification)
        -If two writes are made to a single non-versioned object at the same time, it is possible that only a single event
        notification will be sent
        -If you don't want that you can enable versioning
        Make sure that resource-based of LF is configured true
        S3 notification will send all info in json format about the change

3-Event Source Mapping (invoked synchronously)
    Records need to be polled from the source
    Kinesis Data Streams/ SQS&SQS Fifo/DynamoDB Streams
    -If we configure Lambda to get data from Kinesis, inside Lambda Event Source 
        mapping will be created and and it poll data from Kinesis (e.g.)
        and get batch if data exists then the ESM is gonna invoke the Lambda Function 
        to process the batch of data

    This process can be divided into 2 types:
    1. Stream
        There will be event source mapping creates an iterator for each shard, process items in order
        We can configure weather start with new item, from the beginning or from timestamp
        the processed items are not being removed from the stream. Any data is not effected by the Lambda
        Use case:
        -Low traffic:
            use batch window to accumulate records before processing (staging)
        -High traffic:
            you can configure lambda to process multiple batches in parallel
            up to 10 batches per shard
            in-order processing is still guaranteed for each partition key
        ERROR:
            By default, if function fails, the entire batch is reprocessed until 
            the function is succeed, or the data in batch expires
            You can configure the process:
                Discard old events
                restrict the number of retries
                split the batch on error (to work around Lambda timeout issue)
                (discarded events can be routed to a destination)
    
    2. Queue
        The same with above: Event source Mapping poll data from SQS (fifo) ->
            invokes a LF with event batch
        Poll messages in Long Polling method
        Specify Batch size (max 10,000 for standard, 10 from FIFO)
        
        RECOMMENDED: Set the queue visibility timeout to 6X the timeout of LF
        
        We can use DLQ for problem messages in SQS Service (not Lambda)
        Or set destination to Lambda for failure
        
        LF scales up to the number of active messages group (in fifo)
        
        in in case of standard queue it scales up as quickly as possible
        Lambda adds 60more instances per minute to scale up
        up to 1000 batches of messages processed simultaneously

        the event source mapping might receive the same item from the queue twice
        
        Lambda deletes items from the queue after they are processed successfully

Destinations:
    This is new feature of AWS that store failed or succeed event for reprocess
    Synchronous:
        In this type you can communicate with your LF in real-time, so no need to destination
    
    Asynchronous:
        can be SQS, SNS, Lambda, EventBridge Bus. DLQ as well
        AWS recommends using Destination services instead of DLQ (both can be used at the same time)

    Event Source Mapping:
        for discarded event batches: SQS and SNS
    
    Use case:
        Destination is useful to retain error messages.
        For example we can route the failed events to another lambda function 
        and simultaneously route the success events to SQS Queue

Permissions:
    -We have to specify IAM Role to Lambda Function to get access use other services
    -We have to specify RESOURCE BASED POLICY to Lambda function to give access to other services
    Don't forget that principle does not define a resource but AWS identity (service, user, group or role)

Environment Variable:
    Of course Lambda creates an environment to run the code. 
    And we can assign variables in scale of env.
    - To do so go to configuration -> env. variable and assign variables
    - To get the values in the code, import the `os` and call `os.getenv("<var>")`
    - You can change variable and value without deploying an update
    - You can encrypt (SDK) the variable to keep it secret

Monitoring
    We can see the logs (prints too) and metrics in Lambda console and CloudWatch logs
    Make sure your Lambda function has proper access to log into cloudWatch

    Tracing with X-RAY
        Just enable X-RAY from configuration to run daemon for you->
        Use X-RAY SDK in your code (for customize) -> make sure lambda has access to write in X-RAY
        you can use 3 X-RAY env variable:
            _X_AMZN_TRACE_ID: contains the tracing header
            AWS_XRAY_DAEMON_ADDRESS: the X-RAY Daemon (value is ip:port)
            AWS_XRAY_CONTEXT_MISSING: by default, LOG_ERROR

Edge
    Some application execute smt at the Edge Location
    Edge function is a code you write and attach to CloudFront distribution
    It is run close to the user to minimize latency
    Use cases: You may want to route request intelligently route across origin and data centers
        Website Security and privacy
        Dynamic Web Application
        Search Engine Optimization (SEO)
        Bot Mitigation at the Edge
        Real-time Image Transformation
        User authentication and authorization, etc, etc 
    In this case we have two serverless compute options: 
    1. CloudFront Function
        Lightweight functions written in JavaScript
        For high scale, latency-sensitive CDN customizations
        less ms startup times, millions of request/second
        Used to to change Viewer request and responses (Not origin)

        for example you can deny or allow user according to its token in edge at this point

    2. Lambda@Edge
        But in Lambda@Edge you can customize  origin request/response too
        Lambda function written in NodeJS or Python
        Scales up to thousands of requests/seconds

Networking
    By default Lambda functions is launched outside of your own VPC (in AWS VPC)
    So it cannot access resources in your VPC (RDS, ElastiCache, Internal ELB, etc)
    But it available for Internet and other services in AWS VPC

    Then you can define your own VPC ID (and Subnet, security id) for lambda
    So that you can interact with your sources in the VPC (DB, Storage, ELB etc)
    Check your Lambda role first (should involve AWSLambdaVPCAccessExecutionRole)
    When you define the VPC in Lambda, ENI (Elastic Network Interface) in subnet
    so Lambda function interact with the subnet resources through the ENI
    make sure that the services security group must access the Lambda function

    YES, I got that I must create Lambda function in own VPC to communicate with my VPC 
    sources but in this case how can I access to outside (internet)?
    To do so in case of EC2 we create the instance in private subnet, 
    BUT in case of Lambda functions it does NOT work

    SO what should I do?
    Creating the LF in private subnet gives an internet access if you have a NAT Gateway / Instance
    
    Let's see process:
        Communicate with internet:
            Lambda function in private network connects with NAT in public subnet ->
            NAT sends traffic to INternet through Internet Gateway which located the same public subnet

        Communicate with AWS Services:
            first way is the same with above (through NAT -> IGW) (public connections)
            second way is establish VPC Endpoint so Lambda communicate with AWS like DynamoDB
            directly through VPC Endpoint (this is private connection)
    
    !!!!NOTE: Even you create LF in private subnet CloudWatch still works without any NAT or Endpoint

Performance Configuring
    RAM is configured starting from default value of 128 MB up to 10GB in 1MB increment
    The more you add, the more vCPU credits you get
    at 1729MB, a function has the equivalent of one full vCPU
    in case of more that 1 vCPU consider using multi-threading in your code to benefit from it
    
    Timeout: from 1 to 900 seconds (which means after the period LF doesn't process)

    Execution Context:
        temporary runtime environment that initializes any external dependencies
        Use case: DB connections, HTTP clients, SDK clients
        
        Let's say you want to connect with DB. Are you thinking defining the connection 
            inside function? BAD!!! If you do so, every invocation repeat the same thing.
            Instead you can define the connection part of code out of function. It mountains
            the connection in environment for some time. it is called EXECUTION CONTEXT.
        
        the next execution can re-use the context to execution time and save time in initializing
        connection objects

        Execution Context includes ephemeral storage or /tmp directory (up to 10 GB). 
        It is good to share. Objects are shareable across instances but not functions.
        To encrypt objects in /tmp, generate KMS Data Keys

        If you need permanent file system, use S3

Lambda Layers
    - Custom Runtime such as runtime for C++ or Rust
    - Externalize Dependencies to re-use them:
        Dependencies such as libraries or modules doesn't change often as app packages
        In this case we can externalize the dependencies from the app's packages.
        It can be reused by other functions as well.
        Just create a layer, upload dependency and import it in your code

Storage in lambda
    We have 4 Storage options:
        /tmp         : max 10 GB
        lambda layers: 5 layers per function up to 250 MB total
        S3           : Elastic
        EFS          : Elastic           

    EFS
        You can enable EFS if the VPC is enabled in lambda
        The EFS objects is shared across Lambda Functions
        path/
        Must leverage EFS Access Points

Lambda Concurrency
    concurrency = (average request per second)*(average duration of requests in seconds)
    -> when a function invoked, a environment created and init and lambda handle codes run
    -> during this process there is other invocation, a new env is created and init and handler runs
    -> if both request take average 10 seconds, concurrency = 10*2 = 20

    Reserved Concurrency
        1 region in 1 account can have 1000 concurrency limit
        So be careful to distribute 1000 limit across all functions
        We can specify the number of concurrency limit of instance of a function up to 1000
        To do so set a reserved concurrency at function level
        we can get more than 1k limit by opening a support key
    
    Each invocation over the limit wil trigger a "Throttle"
    Throttle behaviors:
        if synchronous  => return ThrottleError - 429
        if asynchronous => retry and if again so then go to DLQ
            For the Throttle errors(429) and system errors(5xx), Lambda 
                returns the event to the queue and attempts to run the function again
                for up to 6 hours
                
                the retry interval increases exponentially from 1 second after 1st attempt 
                to a maximum of 5 minutes

    Cold Start:
        If you have heavy initial process (out of function), first instance 
            start slow because of initialization (DB connection/etc)
            this is called cold start

    Provisioned Concurrency:
        Allows you to set an initial number of concurrent executions for a function to ensure
        that the function has sufficient capacity to handle incoming requests.

        to avoid Cold Start you can run the first instance in advance that before user do it
        In this case cold start never happens
        Application Auto Scaling can manage provisioning concurrency (schedule or target utilization)

Dependencies
    aws lambda create-function --zip-file fileb://function.zip 
                               --function-name lambda-xray-with-dependencies
                               --runtime nodejs14.x
                               --handler index.handler
                               --role arn:aws:iam::001736599714:role/DemoLambdaWithDependencie
    
    --zip-file: contains all code includes dependencies which run by lambda function
    --handler:  the handler function which is called when LF is invoked

Lambda Function as a code. CloudFormation
    You can define a template that creates a new Lambda Function
    Inline:
        You define code of LF in CloudFunction yaml along with other parameters (runtime, role, etc)
        Used for very small functions   
        Cannot define dependencies
    S3:
        Instead of inline code you can reference to S3 zipped object
        3 parameters you should specify:
            S3Bucket - from which function you want to take the .zip file from
            S3Key    - address of the .zip object in the bucket
            S3ObjectVersioning
                The S3ObjectVersioning parameter accepts a string value. The possible values are:
                    "All": The function uses all versions of the object in the S3 bucket.
                    "Latest": The function uses only the latest version of the object in the S3 bucket.
                    "String": You provide a specific version ID as a string.
        Unless you update one of S3Bucket, S3Key, S3ObjectVersioning the CloudFormation does not update your function
                                       even though you updated the code itself. That is why versioning is recommended
                            
        Multi Account:
            you can reference other account's S3 resource for your CloudFormation
            For this:
                Other account creates a role with GetObject access on its S3 and your CF role is specified as principal
                And our account specify the GetObject access on the S3 resource in the CF role 
                And Other account's S3 Bucket Policy should allow you
                
Lambda Container Images:
    Instead of uploading .zip file you can define own image container
    It packs your up to 10GB of packages (code, dependencies, etc)
    The context and syntaxes is the same with Docker:
        First to communicate with Lambda you get base Lambda Runtime API (Python, Nodejs, custom) image
        And you can add your own code
        Install packages you need (dependencies)
        And they all will be a image
        Just publish it ECR, Docker hub, ...
        Dockerfile:
            # Use an official Node.js runtime as a base image
            FROM amazon/aws-lambda-nodejs:12

            # Copy package.json and package-lock.json to the working directory
            COPY package*.json ./

            # Install dependencies
            RUN npm install

            # Copy the rest of the application code
            COPY . .

            # Set the Lambda function handler
            CMD ["app.handler"]

    What will happen the lambda function is invoked?:
        Container Image Retrieval:
            If the function is not already running or if it needs to be scaled to handle the current load,
            AWS Lambda pulls the container image from the specified container registry (e.g., Amazon ECR, Docker Hub)
        Container Initialization:
            The Lambda runtime initializes the container by executing the instructions in
            the Dockerfile (e.g., installing dependencies, setting up the environment).
        Function Execution:
            The Lambda runtime invokes the specified entry point (e.g., the CMD in the Dockerfile)
            to execute the Lambda function code.
        Processing Event:
            The Lambda function processes the incoming event or trigger based on
            the logic defined in your function code.
        Response:
            The Lambda function can generate a response, which is returned to
            the invoker or passed to other AWS services.
        Container Reuse (Warm Starts):
            If the function is invoked again within a certain period and there is an available
            warm container, Lambda reuses the existing container. This helps to reduce the startup time and improve performance.
        Container Retention (Container Reuse Window):
            AWS Lambda retains the container for a certain period, known as the "container reuse window."
            During this window, if the function is invoked again, Lambda can reuse the existing container
            without the need to pull the container image again.
        Container Cleanup (Cold Starts):
            If the container is not used within the retention period or if it's beyond the container reuse window,
            AWS Lambda may terminate the container to free up resources. Subsequent invocations may result in a
            new cold start, requiring the container image to be pulled again.

Versioning
    You may create several versions of your code. Click "create version" for this
    When you create ur 1st version it's id will be 1 and so on.
    all versions are independent and have different ARN
    But you can give them alias names. Test, Prod, Dev, etc
    And aliases can be Weighted across versions. For example Dev may be Weighted 10% for v3 (or $Latest) and 90% fpr v2
    When you specify id of version function code will be immutable. When you do $latest it will be mutable. 
                                                  codes are changed in function when new versions is available

Function URL:
    Generates dedicated URL endpoint for a lambda function (points to either alias or unpublished version (latest) of the function)
    So, you can invoke the function by web browser, curl, postman, etc
    Access is only through public internet (not Private-link)
    Supports Resource-based Policy and also CORS (Cross-origin ) configuration
    Auth Type (allowing principals' traffic by a security method):
        None - allows public and unauthenticated access
        AWS_IAM - used to authorize or authenticate request by IAM
            The same account:
                Identity-based policy OR Resource based policy required 
            Cross Account
                Identity-based policy AND Resource based policy required

Others:
    limits:
        Memory allocation: 128-10GB (1mb increment)
        Maximum execution time: 900 seconds
        Env variables: 4 KB 
        Lambda function deployment size compressed 50MB or uncompressed 250MB (Code+dependencies)

------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------DynamoDB-------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Serverless NoSQL Data Base (NoSQL - not only SQL or non SQL)
Scale horizontally by additional instance
It is distributed. No joins. No aggregation
each row is up to 400KB

Data types:
    Scalar         - String, Number, binary, Boolean, Null
    Document Types - List, Map
    Set types      - String Set, Number set, Binary list

Primary Key
    It is mandatory to choose a PK for a table during creation
    PK can be in two options:
        Partition Key (or HASH key)
            Should be high cardinality
            Unique user id for example
        Partition Key + Sort Key
            For example a table which records games.
            User id can be partition key, a user can attend more than 2 games
            In this case Game id field will be Sort key.

Table's capacity
    Provisioned
        You specify the number of reads/writes per second              \
        You need to plan capacity beforehand                            |
        Pay for provisioned capacity units                              |
                                                                        | - You can switch diff modes once in 24 hrs
    On-demand                                                           |
        Read/write automatically scale up/down with your workloads      |
        No capacity planning need                                       |
        Pay for what you use, more expensive (2.5x Provisioned)        /

    Write Capacity Units (WCU):
        1 WCU represents 1 write per second for an item up to 1KB
        If the items are larger than 1KB, more WCU are consumed
        
        Sample 1: 20 items are written in a table per second and each item size is 2KB
                  20 * (1 sec / 1 sec) * ceil(2KB) = 40 WCUs
        Sample 2: 120 items per minute, each item 4.5 KB
                  120 * (1 sec / 60 sec) * ceil(4.5KB) = 2 * 5 = 10
    
    Write Capacity Units (RCU):
        When data is written into DB, in backend, it is done to a certain server
        And this server replicates the dato to other read services
        and client is addressed to the other service to read data from.
        REGARDING this process read capacity is divided into 2 types
        
        - Eventually Consistent Read (default)
            It is possible some data staling when requesting just after it is uploaded
            After, for example some milliseconds
        
        - Strongly Consistent Read
            No any data stale.
            Now upload, then you can read it now.
            To apply this option in GetItem, BatchGetItem, Query or Scan APIs, we set `ConsistentRead` as True
            it charges double RCUs of Eventually Consistent Read
        
        Calculating RCU
            1 RCU represents 1 Strongly Consistent Read or 2 Eventually Consistent Read per sec, for an item up to 4KB
            if a item more than 4 KB more RCU will be added
            Sample 1:
                10 Strongly Consistent Read per second, 4KB item size
                10 * 1sec * ceil(4KB/4KB) = 10RCUs
            
            Sample 2:
                10 Eventually Consistent Reads per second with 6KB size:
                (10/2) * 1 sec * ceil(6KB/4KB) = 5*2 = 10 RCUs
                It would be 20 if Strongly Consistent Read option was enabled

Partitions Internally
    Data is stored in partitions
    Partition Keys go through a hashing algorithm to know to which partition they go to
    To compute number of partitions:
        - by capacity = (RCU/3000)+(WCU/1000)
        - by size = TotalSize/10 GB
        - #of partitions = ceil(max(# by capacity, # by size))
    WCUs and RCUs are spread evenly across partitions

Throttling:
    If we exceed provisioned RCU or WCU, we get "ProvisionedThroughputExceededException"
    Reasons:
        How Keys/Partitions - a partition is requested too often
        Very large Items - RCU and WCU depend on size of items
    Solution:
        Exponential backoff (already in SDK)
        Distribute partition keys as much as possible
        If RCU issue, we can use DynamoDB Accelerator (DAX)

Basic Operations
    PutItem
        Creates new item or fully replaces old item
        Consumes WCU
    
    UpdateItem
        Edits existing item's attributes or adds a new item if it doesn't exist
        Can be used to implement Atomic Counter
    
    GetItem
        Read based on Primary Key
        Read mode can be specified
        ProjectionExpression can be specified to retrieve only certain attributes
    
    Reading Data (Query)
        Returns items based on:
            KeyConditionExpression
                Partition Key value (must be = operator) - REQUIRED
                Sort Key value (=, <, Between, Begin with, etc)
            FilterExpression:
                This expression is executed after Query operation just for visualization
                Used only non-key attributes
        
        Returns items specified by LIMIT or up to 1MB
        To get more data you can use pagination in result

        Can query table, Local Secondary Index or Global Secondary Index
    
    Reading Data (Scan)
        Reads entire table
        The result can be filtered only and it is inefficient for capacity
        returns up to 1 MB data and can use pagination
        Consumes a lot of RCUs
        use LIMIT to reduce the cost
        Use PARALLEL SCAN for better performance
    
    DeleteItem
        Deletes an item from table 
        Ability to perform a conditional delete
    
    DeleteTable
        Delete a whole table
        Much quicker than deleting one by one with DeleteItem

    Batch Operations:
        Define multiple API calls in one API call
        Operations are done parallel for better efficiency
        not atomic, port of items can fail

        - BatchWriteItem
            Can write/delete into only one table
            Collection of max 25 PutItem and/or DeleteItem in one call
            up to 16 MB write, up to 400KB for per item
            UnprocessedItems for failed write operations (use exp backoff or add WCU)

        - BatchGetItem
            Can read from multiple table
            Up to 100 GetItem or 16 MB of Data
            UnprocessedItems for failed write operations (use exp backoff or add RCU)

    PartiQL
        SQL-compatible query language for DynamoDb
        select, insert, update, delete
        Can be run in:
            AWS Console
            NoSQL Workbench for DynamoDB
            DynamoDB APIs
            CLI, SDK
    
    Conditional Writes
        For Put, Update, Delete and BatchWrite
        You will specify condition expression to determine which item should be modified
            - attribute_exists
            - attribute_not_exists
            - attribute_type
            - contains (for string)
            - begin_with (for string)
            - IN / BETWEEN
            - size (string length)
    
        Sample
            $aws dynamodb delete-item --table-name ProductCatalog --key '{"ID": {"N": "456"}}' \
                                      -- conditional-expression "attribute_not_exists(Price)"
    
    NOTE!!!!!! FilterExpression is for only result of read operation
               ConditionalExpression is for write operation

Indexes - LSI and GLI
    This is no-SQL. Everything is about: Keys and others
    For querying data you can specify only ONE partition key and multiple Sort Keys
    But sometimes you often need to query based on other non-key attributes
    But this is not directly possible in DynamoDB but you will scan full table

    In this situation you can create indexes so that you can query based on those
    
    Local Secondary Index
        Alternative to Sort Key for a table
        It is scalar attribute
        Up to 5 LSIs can be created per table
        Must be defined in creation
    
    Global Secondary Index
        Creates separate object (table)
        Alternative to Primary key (You can define Partition Key and Sort Key (optional))
        Can be created after table creation
        Have separated throughput capacity
        If GSI throttle ,base table also throttle

Optimistic Locking
    To prevent conflicts of updates
    Every row has VERSION number
    if two request try to update row, one of them is ignored as another updated version number indirectly

DAX
    fully managed cache, memory service of Dynamo
    You will create DAX cluster and specify nodes 
    And your popular data is gonna be cached in DAX cluster
    Good solution for Hot Keys

Streaming
    You can enable Streaming in table for update/put/delete
    it can be used Kineses or can trigger a Lambda function to log data
    Duration: 24 hrs

TTL
    You can enable TTL and specify attribute name
    And during item creation specify epoch number in the attribute
                                        which you set for TTL
    After it is expired data is deleted
    It ensures your data is always fresh
    doesn't consume WCU cost
    Logged in DynamoDB Stream
    Deleted from secondary indexes as well

CLI ($aws dynamodb scan ...)
    --projection-expression "att1, att2, ...": attribute(s) to retrieve
    --filter-expression: filters the result
    --page-size: defines number of items for per API call (to avoid timeout)
    --max-items: number of items shown in CLI (in addition it gives NextToken like pagination)
    --starting-token: takes token which should go to

Transactions (ACID)
    all-or-nothing operation (add/update/delete) across one or multiple operations
    Provides Atomic, Consistency, Isolation and Durability (ACID)
    It is availability in Read and Write mode
    TransactionGetItems - one or more GetItem s
    TransactionWriteItems - one or more Put/Get/DeleteItem s
    Consumes 2x WCU & RCU (because stage and commit operations)
    Capacity Computation:
        1 WCU = 3 Transactional Writes, with item size of 5KB
        1 RCU = 5 Transactional Reads/, with item size of 5KB

Session State Cache
    You can cache Session State in DynamoDB.(Login session for example)
    
    vs Elasticache:
        It is in-memory service while DDB is serverless
        DDB can scale out/in easily
    
    vs EFS:
        EFS is good choice but it is file system while DDB is database
    
    vs EBS:
        It is local, doesn't share its cache to other from EC2 instance 
    
    vs S3:
        It is specialized for big size data and higher latency

--------------------------------------------------------------------------------------------------------------------
----------------------------------------------------API Gateway-------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
Serverless, version
Middleman that receives and sends proxy request (after some modification) to the aws/on-premises resource
It opens our servers to the world of Internet

Integrations
    Mock:
        API Gateway returns a response without sending the request to the backend
      _____________________________________________________________________
    AWS Lambda:                                                            \
        Invoke lambda function                                             |
        Easy way to expose Rest API backed by lambda                       |
                                                                           |
    HTTP:                                                                  |
        Expose HTTP endpoints in the backend (ALB, own on-premises API)    | - can be mapped
        useful for rate limiting, caching, user auth, API keys, etc        |
                                                                           |
    AWS Services:                                                          |
        Expose any AWS API through API Gateway                             |
        Start an AWS Step Function workflow, post a message to SQS         |
      _____________________________________________________________________/    
    
    AWS_PROXY (Lambda Proxy):
        incoming request from the client is the input to Lambda
        No and mapped request to backend or mapped response to client. IT IS Direct
        Cannot add header
    
    HTTP_PROXY:
        No mapping templates
        the HTTP request is passed to the backend (and back response) directly 
        Possibility to add HTTP Headers if need be (API key )

    Mapping template? For example you have an SOAP API (XML) server and but the data comes from
    Rest API (JSON). Thanks for Mapping feature of API Gateway, it fetches data from rest API
    and translate/maps to XML and send proxy request to the SOAP request, and get response from
    the SOAP in XML request and send proxy response to rest API in JSON format
    If you don't want to any transformation, you can enable "proxy integration"

Endpoint Types
    Edge-optimized (default)
        For global clients
        Requests are routed through CloudFront Edge locations 
        API Gateway still lives in only one region

    Regional
        For clients within the same region
    
    Private
        Can only be accessed from your VPC using an Interface VPC Endpoint (ENI)
        Use resource policy to define access
    
Authentication through:
    IAM Roles (internal users)
    Cognito (external users)
    Custom Authorizer (own logic)

Stages:
    Every project has its own several environments like prod/test/dev/etc..
    But how API knows that which user must be routed to which resources?
    We will create stages: dev, test, prod
    Then endpoint will be like: URL/DEV/home/get-product
                                URL/TEST/home/get-product
                                URL/PROD/home/get-product
                
                Use Case
    (Initially we have no any stage yet)
    For example we may want to route users to lambda functions
    function name is get-product and we have 3 alias: dev ->  $Latest version
                                                      test -> 2nd version
                                                      prod -> 1st (95%) and 2nd (5%) version
    In this case, the ARN of the function alias will be almost the same except 
                                                        ':<alias name>' suffix

    Let's back to API Gateway. we will create resource to `/home/get-product`
    And create method: method type: GET, Integration: Lambda Function, proxy: on,
        Lambda function ARN: `<ARN of function>:${stageVariables.lambdaAlias}`
    So it means that given ARN is not completed yet as the part after colon is
    taken by stage variable named lambdaAlias
    Now, create a stage DEV and assign variable lambdaAlias as dev
    Then the ARN in DEV stage will go to function of `<ARN of function>:dev`
    Recap: URL/DEV/home/get-product ---------> `<ARN of function>:dev`

    You learn Stage and Stage Variables right now)

Integration with OpenAPI
    OpenAPI is a kinda definition of API.
    It can be JSON or YAML
    in case of API Gateway you can upload a OPENAPI file to create an API
    Or can export current API as an OpenAI file
    It can be helpful to get API logic easily by either human or machine

Cache
    You cab enable caching in API Gateway. 
    So, the service looks up from cache, if not found, it goes to backend
    TTL is 399 seconds by default. 0-3600s
    Size: 0.5 - 237 GB
    - caches are defined per stage
    - Possible to override cache settings per method
    - Very expensive, makes sense in only production stage
    Caching can be disabled immediately with header: `Cache-Control: max-age=0`
    NOTE!!! If you don't impose an InvalidateCache policy or keep disabled Per-key cache invalidation,
        in console of API/method creation, any client can invalidate the API cache

CloudWatch with API Gateway
    Logs:
        Enabled at the Stage level (wth Log lvl - ERROR, DEBUG, INFO)
        Contains info about request/response body
        Can override settings on a per API basis
    
    X-Ray:
        Enable x-ray to get extra info about requests in API Gateway
    
    Metrics:
        Metrics are by stage. Possibility to enable detailed metrics
        CacheHitCount & CacheMIssCount: efficiency of the cache
        Count: API requests in a given period
        IntegrationLatency: Time btw API send request and get response from backend
        Latency: Time btw API Gateway receives request and send response to a client (indeed greater than IntegrationLatency
                                                                                        smaller than timeout 29 seconds)
        4XXError (client-side) & 5XXError (server-side)
    
    Throttling:
        throttles requests at 10000 rps across all API
        429 Too Many Request error
        Can set Stage limit and method limits

Security
    IAM: create and attach IAM policy to a user/role so that he can invoke an API. 
    Authentication -> IAM & Authorization -> IAM Policy
    Leverages "Sig v4" capability where IAM credential are in headers

    API Gateway Resource Policy
        Defines who/what can access on what in which action.
        Used for Cross Account Access (combined with IAM)
        Allow for a specific source IP addresses /a VPC endpoint
    
    Cognito User Pools
        A database of users
        Fully manages user lifecycle, token expires automatically
        API Gateway verifies identity automatically from AWS Cognito
        Authentication -> Cognito User Pools & Authorization -> API Gateway Methods

    Lambda Authorizer (Custom Authorizer)
        Token-based authorizer (bearer token, Oauth, JWT)
        A request parameter-based Lambda authorizer (header, query string, stage var)
        Lambda must return an IAM policy for the user, result policy is cached
        Authentication -> External & Authorizer -> Lambda function

        For example: a user gets Oauth token from 3rd Authentication system and 
        send request to the APIG's API with the token. the APIG takes token and Context
        and sent it to dedicated Lambda Authorizer so the Lambda authorizer goes to the
        3rd part app to verify the token and return IAM policy according the token (if success)
        After APIG getting IAM policy, save the policy in Policy cache and talk with backend

WebSocket API
    We know that traditional and ordinary APIs response you when you request something
    Like: because you are too shy, you talk to your gf when she asks something
    It is kinda one-way communication. You are, let's say, SOAP API

    But your gf has another bf (i am sorry, but he is not you) who's similar herself:
    sometimes He asks and she answers or sometimes vice-versa.
    In this case your boyfriend-in-law is WebSocket API.

    For the client (I am about your gf again) both APIs are beneficial in different
    purposes. WebSocket is used by the client in real-time scenarios (chat,
    collaboration platforms, etc...) while the Rest API is cost-effective and processes
    more complex tasks ;)

    Two-way interactive communication btw a user's browser and a server
    client and WebSocket API are tied through persistent connection.
    The client is routed among different Lambda functions regarding its request context 
    (onConnect, sendMessage, etc...)
    For example client send request that defines sending message to another client to 
    Websocket API Gateway and the APIG routes the request to proper backend that send 
    the message to the another user. And it can be stored in DynamoDB

    URL: `wss://[some-uniqueId].execute-api.[region].amazon.com/[stage-name]`

    When user call this wss URL first time (onConnect), APIG establishes a new persistent
    connection and every connection (APIG -> Lambda or Lambda -> DynamoDB) gets the same
    `connectionid`. And the connectionid is re-used for next calls

    It is still one-way right? go ahead: the other client reply to you a message.
    In this case a backend sends HTTP Post to you back through the same connection
    by specifying the `connectionid` in the url:
    Post -> `.../[stage-name]/@connection/connectionid`. It is called callback.
    HERE WE GO! Now we have two-way communication with the server. 
                                    ||
                                    \/
    Connection URL Operations:
        Post   -> Sends a message from the server to the connected WS Client
        Get    -> Gets the latest connection status of the connected WS Client
        Delete -> Disconnect the connected client from the WS connection

    Routing:
        When you call the API for sending message how the APIG know which function it
        needs to invoke? By predefined routing
        If no routes, it is routed to $default function
        as a client you define route selection expression on the request says which 
        function should be chosen
        
        Incoming data: {"service": "chat", "action": "join", "data": {"room": "r1234"}}
        our route key table: $connect -> f1, $disconnect -> f2, $default -> f3, join -> f4, etc...
        So the expression can be $request.body.action ("join") so it invokes f4.

Testing Production:
    We always test firstly after releasing a new features. Last step of testing is that 
    testing with real clients in production level.
    To test for example, prod stage in APIG, we will enable Canary in stage level.
    during enabling canary we are asked to specify traffic shares to canary and current stage.
    here, canary is the latest deployed API while current stage is current stage. 
    For example after enabling canary if we deploy a new version of API on the stage
    it is considered as canary and the previous version is still 'current stage'.
    If again deploy 3rd version, this 3rd version is canary and the 1st version is still
    'current stage'. After Promoting Canary in console of stage, the 3rd version will be 
    'current stage' now

--------------------------------------------------------------------------------------------------------------------
----------------------------------------------------CI/CD-------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------

Continuous Integration - 
Code Delivery - 

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ CodeCommit @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Just alternative of GitHub and AWS-native
Codes are encrypted and and have access control
Integrated wit Jenkins, AWS CodeBuild, and other tools

Authentication:
    SSH Keys - can be configured in IAM console


    HTTPS - with AWS CLI Credential helper or Git Credentials for IAM user
        in IAM console go to "Security credentials" and go AWS CodeCommit credentials
        from "HTTPS Git credentials for AWS CodeCommit" click "generate"
        It will generate a username and password
        Go to CodeCommit code section and choose HTTPS clone so you'll get url
        Type in tour local terminal: `git clone <the url>`
        And you will asked username and password which is generated by IAM
    
Authorization:
    IAM policies to manage users/roles permissions to repositories

Cross Account Access:
    Use IAM Role and AWS STS (AssumeRole)

Settings:
    Notification
        We can define which action we must be aware of
        notification is sent through SNS Topic, ChatBot, Chatbot in Teams

    Trigger:
        In specific action GitCommit trigger a lambda function or send message to SNS 

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ CodePipeline @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
It is orchestration that visualize every stage from the code source to production server

Stage Types in creation:
    - Source -> source of code (CodeCommit, Github, ...)
    - Build  -> builds your code and do tests (optional, CodeBuild, Jenkins)
    - Deploy -> end-servers which you code should run in (Beanstalk, S3, ECS, ...)
Initially we are asked to specify an action per the 3 stages (Build one is optional)
And we can add action/ action groups to existing stages or add extra stages by EDITING
The stage types are actually action types or action providers
and we have other action providers as well
    - Test   -> CodeBuild, aWS Device Farm, 3rd party tools
    - Invoke -> Lambda, step functions
You can add many stages by editing the pipeline:
new stage -> it can contain many sequential "action group"s which each contains parallel actions

Artifacts
    Each pipeline stage can create artifacts
    Artifacts stored in an S3 bucket and passed onto the next stage
    for example , first source stage is triggered and the result is saved in S3 as a artifact
    then the next stage is triggered and given input based on the saved artifact, so on
    every transportation (output/input) artifacts are saved in S3

For example, let's say you have Elastic Beanstalk application which contains dev and test envs
what you want to do is that when main branch of the repo is updated your update should run in
dev environment of the beanstalk app. And after manual approving of a user it should be concluded
that everything is ok and it can be deployed on the prod environment
                           ||                      ||
                           \/ Let's make it happen \/

In this case we need 4 stages:
    - "Source"
        in this stage we have only one action group and in it only one `source` action
        that fetch code from our proper branch and it returns output artifact
    
    - "Development"
        in this stage also we have a action group -> `deploy` action that (takes input artifact
        from the source stage) deploys the code to dev env of the EB app
    
    - "Production"
        In this case we need 2 action groups
            - "Approving" which includes a `manual approval` action. If this action is approved
              the next action group will be triggered"
            
            - "deployToProd" which includes a `deploy` action or action provider that deploys
              code from the 2nd stage to the prod env of the app
    
    We can add other stages that include code build or test actions to enrich our pipeline

Monitoring:
    Use CloudWatch Events to (for example)
        create events for failed pipelines
        create events for cancelled stages (like when you REJECT manual approval action)

Accesses are defined in service IAM role for pipeline

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ CodeBuild @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Well, thus far, we automated deployment from CodeCommit to dev/prod envs.
But in this process, only the Manual Approval is logical filter to protect our prod env from wrong code
As a human being the person who dedicated to approve can be still mistaken
In this case at middles we should test the codes through predefined tests yes? Of course!
First of all, codes must meet some requirements defined in tests. But WHERE and HOW the codes 
runs to run on test? in CodeBuild

CodeBuild ups docker images/containers at the backend to run tests against our changed codes
We can use either AWS-managed images or Custom images (for example we want run our app on Alpine OS)
In these cases we create CodeBuild project. And add it to our pipeline as testing filter

CodeBuild builds the code according to `buildspec.yml` at root of file system.
(For example we define the same logic in .github/workflows.yaml file in case of GITHUB)
instead of `buildspec.yml` file the commands can be defined in web console but not recommended

version: 0.2

phases: 
    install:
        runtime-versions:
            nodejs: latest
        commands:
            - echo "installing something"
    pre_build:
        commands: 
            - echo "we are in the pre build phase"
    build:
        commands:
            - echo "we are in the build block"
            - echo "we will run some tests"
            - grep -Fq "Congratulations" index.html
    post_build:
        commands:
            - echo "we are in the post build phase"
artifacts:
    files:
        - ...
cache:
    paths:
        - ...
This is example for the `buildspec.yml` file.
If "Congratulations" text is found in index.html file it succeed, otherwise raise error
And this error blocks workflow of the pipeline
The logic is that in the index file this text must be regardless where it is located inside of file

The logs can be stored in S3 & CloudWatch
Metrics  - to monitor statistics
EventBridge - to detect failed builds and trigger notification
Alarm - to notify if you need "thresholds" for failures

Caching
    If something is reusable you can cache it into S3
    Dependencies for example

Artifacts:
    which file in docker container should be sent to S3

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ CodeDeploy @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Deployment service that automates application deployment v1 -> v2
New version may be deployed to EC2 Instances, On-premises, Lambda functions or ECS Services
Automatically rollback if any failure occurs, or trigger CloudWatch Alarm
appspec.yml defines how the deployment happens

CodeDeploy with EC2/On-premises
    Perform in-place or blue/green deployments
    Must run in the CodeDeploy Agent on the target instance
    Define deployment speeds
        In-place
            - AllAtOnce: all instances stop and got deployed and start
            - HalfAtATime: half of instances stop and got deployed and start, 
            and the same with the last half (kinda in-place)
            - OneAtATime: one instance stops then got deployed and starts, the same process
            until there is no ant lasted instance
            - Custom: you can define your % of instances' share for deployment process
        Blue / Green
            a new group of instances is created as an alternative of version 1
            it can be ALB, new Beanstalk env, whatever that alternative of current System
            and after some testing the url of Route53 is just SWAPPED 

    CodeDeploy Agent
        it must be installed in an instance to act like deployer behalf of your instance
        The instance must have sufficient permissions to communicate with services for deployment
        For example permission to S3 to get deployment bundles (.zip contains new version)

        Go ./Lab.txt file for hands on
Lambda:
    Can help you to automate traffic shift for Lambda aliases
    this is integrated within the SAM frameworks
    for example: Prod alias goes to V1 and we have newer version v2
    Prod ---- 100-x% ---> v1
        `------ x% -----> v2  it is used to increment the value of x
    
    The increment methods:
        Linear: grow x every N minutes until x=100%
        Canary: try x percent then 100%
        AllAtOnce: immediately 100%

ECS:
    Logic is almost the same: there will be ALB that goes to ECS Task Definition
    A new ECS Task is created inside the same ECS Cluster - V2
    Code Deploy change the route to the new ECS Task Definition step-by-step
    
    Only Blue / Green Deployment is available
    And increment method are Linear, Canary, AllAtOnce

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ Others @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

CodeStar
    just creates CI/CD project
    For example choose django web app and create the project
    It will create all objects for you (IAM roles, CodeCommit, Pipeline, Build, EC2, etc...)

CodeArtifact:
    We don't want to our services/users within the VPC installing dependencies from internet
    So we can create/open repositories in CodeArtifact (repoForJavaDevelopers, repoForPythoners, etc)


CodeGuru
    Automated machine learning based code reviewer
    Detects issues, minor bugs or can identify cost effectiveness in server running code
    it is available in even On-premises
    To use install its Agent on your machine

Cloud9
    Cloud-based IDE (like. VS Code)
    Code editor, debugger, terminal in a browser
    Share yor development env with your team
    Integrated with AWS SAM & Lambda to easily build serverless apps
    It runs in EC2


--------------------------------------------------------------------------------------------------------------------
---------------------------------------------------- SAM -------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
SAM = Serverless Application Model
Framework for developing and deploying serverless application
    - Lambda Function
    - API Gateway
    - DynamoDB tables
All configuration is defined as YAML
The yaml file contains "AWS::Serverless-2016-10-31" so CloudFormation 
    thinks that it is SAM configuration and create complex templates according
    to it

Well, logic is very sample:
You are working in your local machine for example VS Studio
And you need to create/update serverless system (Lambda, DynamoDB, API Gateway)
usually it is done in cloudformation, so cloudformation give us a cool feature: SAM
We create a yaml file in our local and define the services with codes in local
you can see it in test-sam/ folder, I commented everything there
And you should run the commands in commands.sh file in the folder. (you must be connected to aws via ssh) 
you can do it all by just `sam init ...` (don't forget install SAM CLI for that), It will give sample templates

Everything looks like git deployment: Github -> S3, a server -> cloudformation
git commit ==> aws cloudformation package (or sam package)
deployment ==> aws cloudformation deploy (or sam deploy)

Here is some SAM Policy Templates (that can be specified what your function can do):
    - S3ReadPolicy
    - SQSPollerPolicy
    - DynamoDBCRUDPolicy
    for example:
        ```
        MyFunction:
            Type: 'AWS::Serverless::Function'
            Properties:
                ...
                Policies:
                    - SQSPollerPolicy:
                        QueueName:
                            :GetAtt MyQueue.QueueName
        ```

SAM nad CodeDeploy:
    Well now we know that we can create/upload functions via SAM, but how about deploying?
    JUST ADD these lines:
        ```
        AutoPublishAlias: live
        DeploymentPreference:
           Type: Canary10Percent5Minutes 
        ```
    Go to ~/Documents/AWS/developer-associate/sam-codedeploy folder for further info
    the sam-app folder is generated after `sam init ...` command. Don't confuse

sam build   -> fetch dependencies and create local deployment artifacts
sam package -> package and upload to S3, generate CF template
sam deploy  -> deploy to CF

--------------------------------------------------------------------------------------------------------------------
---------------------------------------------------- CDK -------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
CDK is almost the same stuff with SAM: You define infrastructure as a code
SAM differs with that it focuses on only serverless and the code is defined as yaml
But with CDK, you will define your code in many PLs against any service of AWS
                                    and it is compiled to CF template like SAM
CDK contains high level components called constructs

!!! You can see all hands-on in ./test-cdk-python/cdk-app folder
First go to empty folder and say : `$ cdk init --language python`
It generates cdk-based template for you
So start defining stacks in the python file (it is created with template like "Cdkappstack")
in the folder of python files you can create python files which each contains a stack 
so a stack contains many services as you want
After defining all services and stacks back to amin folder and create app.py file 
which says to CDK: hey generate CloudFormation template according to me
why app.py? because it is written in cdk.json file which is backbone of CF template
When you command `$ cdk synth` (do it in main folder) it goes to cdk.json and the json
says hey go to app.py file to know which services is need to be created. So backend goes to 
app.py and the file says hey here is stacks (it is imported from the stacks folder)
and generate templates according to this stack classes, and it will be generated
When you  then command `$ cdk deploy` the change will be deployed into the CloudFormation
(the first time don't forget to command `$ cdk bootstrap` for per region in an account
it deploys the CDK Toolkit staging Stack)
command `$ cdk destroy` to destroy the stack
command `cdk diff` is used to see difference bte current and in cloud stacks

By bootstrapping, we deploy the CDKToolkit staging Stack, which our deployment is
processed before going into AWS environment (the toolkit is available in SAM as well)




--------------------------------------------------------------------------------------------------------------------
---------------------------------------------------- Cognito -------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
Give users an identity to interact with our web or mobile app
Cognito: "hundreds of users", "mobile users", "authenticate with SAML"
it identities out of AWS users while IAM does for AWS users

Cognito User Pools:
    Sign in functionality for app users
    integrated with API Gateway and ALB

    CUP has serverless db of users
    Handles login through username/email with password
    has email and phone number verification
    MFA. Federated identities: Facebook, Google, etc
    It sends back a JSON Web Token or JWT


Cognito Identity Pools (Federated Identity):
    Provide AWS credentials to users so they can access AWS resources directly
    Integrated with Cognito User Pools as an identity provider

    Well, case: We want users to use our private S3 bucket.
        In this case we can't give them IAM roles as they are too many
        Instead we can use Cognito Identity Pools (CIP)

        How it works?
        A mobile app will sign in and get token from CIP identity tools (SAML, OpenID Connect,
        Amazon, Google, Facebook, Cognito USER POOL, etc...), let's take CUP.
        Now user has token from CUP right? Then user will be sent request with the Token
        to Cognito Identity Pool and CIP validates if the token is OK. If so, it gets 
        temporary credentials from STS and back to the user the Credentials
        So now user can access to the defined resource (s3 bucket)

        How CIP knows which accesses the user should be provided? 
        By IAM role that we predefined for the CIP

    IAM:
        default role  -> authenticated and guest users
        defined roles -> by users' IDs
        We can define policy variables on policies
            for example we want to give access to user on prefix which 
            start with user ID:
            ...
            "Condition": {"StringLike": {"s3:prefix": ["${cognito-identity.amazonaws.com:sub}/*"]}} 
            ...


CUP + CIP = authentication + authorization

--------------------------------------------------------------------------------------------------------------------
----------------------------------------------- Step Functions -------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------

AWS Step Functions is a serverless orchestration service that allows you to coordinate 
multiple AWS services into serverless workflows. You can design workflows that 
orchestrate the execution of multiple AWS Lambda functions, as well as other AWS 
services like Amazon S3, Amazon DynamoDB, Amazon SQS, and more.
    Workflow Orchestration
    Data Processing Pipelines
    Microservices Orchestration
    Automated Business Processes

Here's how it works:
    Define State Machine: 
        You define your workflow as a state machine using JSON-based 
        Amazon States Language. This language allows you to define a series of states and 
        the transitions between them.

    States:
        Each state represents a single task or activity within your workflow. States can be 
        one of several types, including Task states (which invoke a Lambda function or an 
        AWS service), Choice states (which allow you to branch the workflow based on conditions), 
        Wait states (which add delays to the workflow), and more.

    Transitions: 
        Transitions between states are defined based on the outcomes of the previous state's 
        execution. For example, a Choice state might transition to different states based on 
        the result of a comparison.

    Execution: 
        Once you've defined your state machine, you can execute it by providing input data. 
        Step Functions will then execute each state in the workflow according to the defined 
        transitions until the workflow completes.

    Error Handling: 
        Step Functions provides built-in error handling, allowing you to define how your workflow 
        should respond to errors or exceptions that occur during execution. You can specify retry 
        logic, catch and handle specific error types, and define fallback paths for when errors occur.

        Well, in a task we have 2 error handlers:
            Retry:
                It defines how the same task should be run after failing:
                ...
                "Retry": [
                {
                    "ErrorEquals": [
                    "CustomError"          --> If error string is "CustomError" then it starts to work
                    ],
                    "IntervalSeconds": 1,  --> After failing after how many seconds it should be retried
                    "MaxAttempts": 2,      --> How many after failing it should be retried
                    "BackoffRate": 2       --> Exponential backoff rate = 2 x IntervalSeconds
                },
                ...]

            Catch:
                What if no success after retries? The error goes to Catch logic:
                LambdaInvoke: {
                    ...
                    "Catch": [
                    {
                        "ErrorEquals": [
                        "CustomError"
                        ],
                        "Next": "CustomErrorFallback",  --> it goes to this state after unsuccess retries
                        "ResultPath": "$.fuckingerror"  --> This add a parameter named fuckingerror to its output,  
                },                                                       \so it will be added to input of next state
                    ...]
                }
                "CustomErrorFallback": {   --> this is the state that error comes here to be handled
                "Type": "Pass",
                "Result": "This is a fallback from a custom lambda function exception",
                "ResultPath": "$.fuckingerrorADD"       --> This also adds fuckingerrorADD param to its output. its value 
                "End": true                                                         will be "Result"'s value
                },...

            Apart from "CustomError" Step function has its prebuilt error strings:
                State.TaskFailed   -> if the parent table fails with unknown reason
                State.All          -> all error
                States.Timeout
                States.NoChoiceMatched
                States.Runtime
                


    Visibility and Monitoring: 
        AWS Step Functions provides visibility into the execution of your workflows through its 
        management console and CloudWatch Logs integration. You can monitor the progress of
        executions, view execution history, and track performance metrics.

    Wait for Task Token:
        It is a special feature of Step Function that tell to the State that "Wait for task token"
        It waits until receiving either SendTaskSuccess or SendTaskFailure API call
        Just append ".waitForTasToken" to "Resource" arn value
        For example, to authorization purpose we can set "Check Credit" State at the start
        and we'll apply the feature to this state. This state waits until the one of the 2 API 
        call received.

    Activity Task:
        In AWS Step Functions, activities are a way to associate code running somewhere (known as an activity worker)
        with a specific task in a state machine. You can create an activity using the Step Functions console,
        or by calling CreateActivity. This provides an Amazon Resource Name (ARN) for your task state. Use this ARN to
        poll the task state for work in your activity worker.

        An activity worker can be an application running on an Amazon EC2 instance, an AWS Lambda function,
        a mobile device: any application that can make an HTTP connection, hosted anywhere. When Step Functions
        reaches an activity task state, the workflow waits for an activity worker to poll for a task. An
        activity worker polls Step Functions by using GetActivityTask, and sending the ARN for the related
        activity. GetActivityTask returns a response including input (a string of JSON input for the task) and
        a taskToken (a unique identifier for the task). After the activity worker completes its work, it can
        provide a report of its success or failure by using SendTaskSuccess or SendTaskFailure. These two calls
        use the taskToken provided by GetActivityTask to associate the result with that task.


--------------------------------------------------------------------------------------------------------------------
----------------------------------------------- AppSync -------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------

AWS AppSync enables developers to connect their applications and services to data and events with secure,
serverless and high-performing GraphQL and Pub/Sub APIs. You can do the following with AWS AppSync:
    Access data from one or more data sources from a single GraphQL API endpoint.
    Combine multiple source GraphQL APIs into a single, merged GraphQL API.
    Publish real-time data updates to your applications.
    Leverage built-in security, monitoring, logging, and tracing, with optional caching for low latency.
    Only pay for API requests and any real-time messages that are delivered.

GraphQL:
    Let's say you have many data source types: Aurora (SQL), DynamoDB (NoSQL), OS, other Custom sources with AWS Lambda.
    And your app need to get access all that data with one API call. 
    In this case we will create GraphQL schema. And define proper data sources
    And we can call to GraphQL regardless of where the data is located
    And GraphQL gets/posts/etc data by API call from its resolvers (data sources)

    Recap:
        Your server (Web/Mobile apps, real-time dashboard, offline sync, etc) request data from GraphQL
        and does'n care about data location. Then AppSync gets the data from sources/HTTP APIs
    
Security & Authentication:
    API_KEY
    AWS_IAM
    OPENID-CONNECT
    Cognito User POOl

Recommenced: For custom domain & HTTPS, use CloudFront in front of AppSync

YOu cn integrate it with your app:
    JavaScript
    Android
    IOS


--------------------------------------------------------------------------------------------------------------------
----------------------------------------------- Amplify -------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
Amplify is a services that allows you create a whole Web/Mobile app. You can do it 
The service provide all needed tools for an app
Backbones:
    Amplify Studio --> it is central console
    Amplify CLI    --> cli commands
    Amplify libraries --> connect your app existing AWS Services
    Amplify Hosting --> Hosting app's front securely, reliable, fast web apps, etc...

$ amplify init  --> prepare / initializes all app stuffs
It gives features such as data storage, authentication, storage, machine-learning.
    all provided by AWS


Backend --> DynamoDB, AppSync, Cognito, S3.
Amplify deploy Stacks on CloudFormation. It will create needed resources
For example for data storing, you will create Data Model in Amplify Studio
But in back end, GraphQL and DynamoDB will be created to store and access your data

Frontend --> provides libraries with ready-to-use components (React, Vue, Flutter, ...)


Important Features:
    $ amplify add auth 
        Leverages Cognito as authenticator
        user registration, auth, recovery and other operations
        Supports MFA, social Sign-in, etc
        prebuilt UI components
        Fain-grained Authorization
    
    $ amplify add api
        Adds data store (DynamoDB, AppSync)

